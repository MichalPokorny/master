Requirements
===

All Python code uses Python 3.

```
pip install --user requests SPARQLWrapper protobuf nltk gflags scikit-learn kazoo etaprogress matplotlib terminaltables
```

Building articles HBase table
==

1. Copy Wikipedia plaintext to HDFS: `hdfs dfs -copyFromLocal data/wiki-plain-backup.txt /user/prvak/wiki-plain/wiki-plain.txt`
2. Create HBase table:
   ```
   $ hbase shell
   # 'wiki' is created column family
   splits = ('A'..'Z').map { |x| ('a'..'z').map { |y| (x + y) } }.inject(&:+)
   splits += ('0'..'9').to_a
   create 'prvak:wiki_articles', 'wiki', SPLITS => splits
   # hbase.hregion.max.filesize is 10 GB by default. want smaller.
   alter 'prvak:wiki_articles', 'hbase.region.max.filesize' => (256 * 1024 * 1024)
   # => ok, creates regions
   ```

To destroy HBase table
==
```
$ hbase shell
disable 'prvak:wiki_articles'
drop 'prvak:wiki_articles'
```

PRA
==
# to run PRA: (instructions for downloading dataset etc.: http://rtw.ml.cmu.edu/emnlp2015_sfe/)
~/bin/sbt 'run examples sfe_bfs_pra_anyrel'  # select 1
~/bin/sbt 'run examples sfe_bfs_pra_anyrel'  # select 2

~/bin/sbt 'run prvak'

Using PBS
===

```
# kill PBS job:
qstat -u prvak
qdel 1309042.wagap.cerit-sc.cz
```

PBS dependency lists: http://beige.ucs.indiana.edu/I590/node45.html

qsub -W depend=afterok:$FIRSTJOBID


Useful commands
===
How to build protobuf files:

```
genrule(
    name = "sentence_proto",
    srcs = [
	"sentence.proto",
    ],
    outs = [
	"sentence_pb2.py",
	"Sentence.java",
    ],
    visibility = ["//visibility:public"],
    cmd = "protoc -I=. " +
	"--python_out=$(GENDIR)/ " +
	"--java_out=$(GENDIR)/ " +
	"$(location :sentence.proto)"
)
```

To download charts:
```
scp -r hador.ics.muni.cz:/storage/brno7-cerit/home/prvak/charts
```

Zookeeper files
===

`/user/prvak/thesis/wikidata-service`: contains `hostname:port` of Wikidata
Fuseki (e.g., `hador:3030`)

`/user/prvak/thesis/dbpedia-service`: contains `hostname:port` of DBpedia
Fuseki (e.g., `hador:3030`)

`/user/prvak/thesis/spotlight-annotators`: list of annotator URLs (e.g.,
`http://lex5.ncbr.muni.cz:2222/rest/annotate,http://lex6.ncbr.muni.cz:2223/rest/annotate`)

Various tidbits
===

Fuseki:
	544253967G
	Broken line: 544253967 (wtd:P487 "\a" ;)
		smazat sedem

	then: 544253980G
		removed entire fact

English wikipedia: 5,213,378 articles
	to NLPize 1 article: 3 minutes

	parallelized in 100 jobs: takes 2606 hours
	parallelized in 1000 jobs: takes 260 hours, ~10 days
	parallelized in 10000 jobs: takes ~1 day

qsub -l walltime=4:00:00 -l nodes=1:ppn=1,mem=1gb -m abe metacentrum_prepare.sh
