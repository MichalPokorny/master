module add python34-modules-gcc
pip install requests SPARQLWrapper protobuf nltk gflags --user

# interactive for Spotlight
qsub -l walltime=4:00:00 -l nodes=1:ppn=16,mem=16gb -I
	# (run spotlight)

# interactive for Fuseki
qsub -l walltime=4:00:00 -l nodes=1:ppn=4,mem=32gb -I

qsub -l walltime=24h -l nodes=1:mem=2gb -m abe annotate_coreferences

# load Wikidata into Fuseki
# TODO: more CPUs?
# 4 GB is not enough.
qsub -l walltime=48h -l nodes=1:ppn=4,mem=16gb:brno -m abe wikidata_into_fuseki.sh

# -m e: send mail when job terminates

# Download dependencies.
qsub -l walltime=4:00:00 -l nodes=1:ppn=1,mem=1gb -m e metacentrum-prepare.sh

# Split wiki.
qsub -l walltime=24:00:00 -l nodes=1:brno:ppn=1,mem=1gb -m abe metacentrum_split_wiki.sh

# NLPize articles.
# qsub -l walltime=1:00:00 -l nodes=1:ppn=4,mem=8gb -m e nlpize_articles.sh
# qsub -l walltime=24:00:00 -l nodes=1:ppn=8,mem=96gb -m e nlpize_articles.sh
qsub -l walltime=48:00:00 -l nodes=1:ppn=32,mem=320gb -m e nlpize_articles.sh

# Spotlight.
qsub -l walltime=1:00:00 -l nodes=1:ppn=1,mem=1gb -m e metacentrum_spotlight.sh

# get_sentences_entities
qsub -l walltime=1:00:00 -l nodes=1:ppn=1,mem=1gb -m e metacentrum_get_sentences_entities.sh

# get_training_samples
launch_get_training_samples

TODO: will need to rerun:
	- wiki to plaintext (probably aborted)

# to run PRA: (instructions for downloading dataset etc.: http://rtw.ml.cmu.edu/emnlp2015_sfe/)
~/bin/sbt 'run examples sfe_bfs_pra_anyrel'  # select 1
~/bin/sbt 'run examples sfe_bfs_pra_anyrel'  # select 2

# kill PBS job:
qstat -u prvak
qdel 1309042.wagap.cerit-sc.cz

PBS dependency lists: http://beige.ucs.indiana.edu/I590/node45.html

qsub -W depend=afterok:$FIRSTJOBID

Fuseki:
	544253967G
	Broken line: 544253967 (wtd:P487 "\a" ;)
		smazat sedem

	then: 544253980G
		removed entire fact

	./fuseki-server --loc /storage/brno7-cerit/home/prvak/data/fuseki-datasets/wikidata /wikidata

English wikipedia: 5,213,378 articles
	to NLPize 1 article: 3 minutes

	parallelized in 100 jobs: takes 2606 hours
	parallelized in 1000 jobs: takes 260 hours, ~10 days
	parallelized in 10000 jobs: takes ~1 day

bazel run //src/tools/generate_workspace -- \
	--artifact=org.apache.hadoop:hadoop-common:2.6.2 \
	--artifact=org.apache.hadoop:hadoop-mapreduce-client-core:2.6.2 \
	--artifact=edu.stanford.nlp:stanford-corenlp:3.6.0 \
	--artifact=edu.stanford.nlp:stanford-corenlp:models:3.6.0 \

tdbloader: more RAM improves speed!
	I want an in-memory TDB database...!
	exceeded limit 4096 MB
