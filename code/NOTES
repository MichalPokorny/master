hador.ics.muni.cz: Hadoop frontend

export HADOOP_CLASSPATH=/storage/brno7-cerit/home/prvak/bin/stanford-english-corenlp-2016-01-10-models.jar
hadoop jar /storage/brno7-cerit/home/prvak/bin/CoreNLP_deploy.jar /user/prvak/wiki-splits /user/prvak/article-parse-xmls-200 200

module add python34-modules-gcc
module add jdk-8
pip install requests SPARQLWrapper protobuf nltk gflags --user

qsub -l walltime=4:00:00 -l nodes=1:ppn=1,mem=1gb -m abe metacentrum_prepare.sh

# interactive for Spotlight
qsub -l walltime=4:00:00 -l nodes=1:ppn=16,mem=16gb -I
	# (run spotlight)

# interactive for Fuseki
qsub -l walltime=4:00:00 -l nodes=1:ppn=4,mem=32gb -I

qsub -l walltime=24h -l nodes=1:mem=2gb -m abe annotate_coreferences

# load Wikidata into Fuseki
# TODO: more CPUs?
# 4 GB is not enough.

# qsub -l walltime=48h -l nodes=1:ppn=4,mem=16gb:brno -m abe wikidata_into_fuseki.sh
#	=> 48h is not enough
qsub -l walltime=96h -l nodes=1:ppn=4,mem=16gb:brno -m abe wikidata_into_fuseki.sh
	=> 12192098.arien.ics.muni.cz
	# data phase finished
	# index phase ran out of memory

qsub -l walltime=24h -l nodes=1:ppn=4,mem=32gb:brno -m abe wikidata_into_fuseki.sh
	=> 12216763.arien.ics.muni.cz

# -m e: send mail when job terminates

# Download dependencies.
qsub -l walltime=4:00:00 -l nodes=1:ppn=1,mem=1gb -m e metacentrum-prepare.sh

# Split wiki.
qsub -l walltime=24:00:00 -l nodes=1:brno:ppn=1,mem=1gb -m abe metacentrum_split_wiki.sh

# NLPize articles.
# qsub -l walltime=1:00:00 -l nodes=1:ppn=4,mem=8gb -m e nlpize_articles.sh
# qsub -l walltime=24:00:00 -l nodes=1:ppn=8,mem=96gb -m e nlpize_articles.sh
qsub -l walltime=48:00:00 -l nodes=1:ppn=32,mem=320gb -m e nlpize_articles.sh

# Spotlight.
qsub -l walltime=1:00:00 -l nodes=1:ppn=1,mem=1gb -m e metacentrum_spotlight.sh

# get_sentences_entities
qsub -l walltime=1:00:00 -l nodes=1:ppn=1,mem=1gb -m e metacentrum_get_sentences_entities.sh

# get_training_samples
launch_get_training_samples

TODO: will need to rerun:
	- wiki to plaintext (probably aborted)

# to run PRA: (instructions for downloading dataset etc.: http://rtw.ml.cmu.edu/emnlp2015_sfe/)
~/bin/sbt 'run examples sfe_bfs_pra_anyrel'  # select 1
~/bin/sbt 'run examples sfe_bfs_pra_anyrel'  # select 2

# kill PBS job:
qstat -u prvak
qdel 1309042.wagap.cerit-sc.cz

PBS dependency lists: http://beige.ucs.indiana.edu/I590/node45.html

qsub -W depend=afterok:$FIRSTJOBID

Fuseki:
	544253967G
	Broken line: 544253967 (wtd:P487 "\a" ;)
		smazat sedem

	then: 544253980G
		removed entire fact

	./fuseki-server --loc /storage/brno7-cerit/home/prvak/data/fuseki-datasets/wikidata /wikidata

English wikipedia: 5,213,378 articles
	to NLPize 1 article: 3 minutes

	parallelized in 100 jobs: takes 2606 hours
	parallelized in 1000 jobs: takes 260 hours, ~10 days
	parallelized in 10000 jobs: takes ~1 day

bazel run //src/tools/generate_workspace -- \
	--artifact=org.apache.hadoop:hadoop-common:2.6.2 \
	--artifact=org.apache.hadoop:hadoop-mapreduce-client-core:2.6.2 \
	--artifact=edu.stanford.nlp:stanford-corenlp:3.6.0 \
	--artifact=edu.stanford.nlp:stanford-corenlp:models:3.6.0 \

tdbloader: more RAM improves speed!
	I want an in-memory TDB database...!
	exceeded limit 4096 MB

# Parse
hadoop jar /storage/brno7-cerit/home/prvak/master/code/bazel-bin/hadoop/CoreNLP.jar CoreNLP -D prefix_length=1000 -D mapreduce.map.memory.mb=9000 -D mapreduce.map.java.opts=-Xmx8000m -D mapreduce.task.timeout=6000000 -D mapreduce.input.fileinputformat.split.maxsize=1000000 /user/prvak/wiki-splits /user/prvak/article-parse-xmls-1000
	=>

# Spotlight
hadoop jar /storage/brno7-cerit/home/prvak/master/code/bazel-bin/hadoop/SpotlightAnnotator_deploy.jar -D prefix_length=1000 -D spotlight_server=http://hador:2222/rest/annotate -D mapreduce.input.fileinputformat.split.maxsize=1000000 /user/prvak/wiki-splits /user/prvak/article-spotlights-separate-1000

hadoop jar /storage/brno7-cerit/home/prvak/master/code/bazel-bin/hadoop/RandomSelector_deploy.jar -D selection_probability=0.001 /user/prvak/wiki-splits /user/prvak/wiki-splits-sub-0.001

# launch on hador
export HADOOP_CLASSPATH=/storage/brno7-cerit/home/prvak/master/code/bazel-bin/hadoop/DocumentProcessor_deploy.jar
hadoop jar /storage/brno7-cerit/home/prvak/master/code/bazel-bin/hadoop/DocumentProcessor.jar DocumentProcessor -D spotlight_server=http://hador:2222/rest/annotate -D mapreduce.input.fileinputformat.split.maxsize=1000000 -D mapreduce.map.java.opts=-Xmx8000m -D mapreduce.map.memory.mb=9000 -D mapreduce.task.timeout=6000000 /user/prvak/wiki-splits-sub-0.001 /user/prvak/wiki-processed-sub-0.001
# has very long bootup

14..19 minut na 1/1000 Wikipedie

hadoop jar /usr/lib/hadoop-mapreduce/hadoop-streaming.jar -input /user/prvak/wiki-processed-sub-0.001 -output /user/prvak/wiki-samples-sub-0.001 -mapper /storage/brno7-cerit/home/prvak/master/code/bazel-bin/mr_generate_training_samples

hadoop jar /usr/lib/hadoop-mapreduce/hadoop-streaming.jar -input /user/prvak/wi-processed-sub-0.001 -output /user/prvak/wiki-samples-sub-0.001 -mapper mr_generate_training_samples.py -files annotate_coreferences.py,dbpedia.py,file_util.py,get_training_samples.py,json_cache.py,parse_xmls_to_protos.py,paths.py,relations.py,sentence_pb2.py,sparql_client.py,training_samples_pb2.py,wikidata_util.py,mr_generate_training_samples.py

hadoop jar -files annotate_coreferences.py,dbpedia.py,file_util.py,get_training_samples.py,json_cache.py,parse_xmls_to_protos.py,paths.py,relations.py,sentence_pb2.py,sparql_client.py,training_samples_pb2.py,wikidata_util.py,mr_generate_training_samples.py /usr/lib/hadoop-mapreduce/hadoop-streaming.jar -input /user/prvak/wi-processed-sub-0.001 -output /user/prvak/wiki-samples-sub-0.001 -mapper mr_generate_training_samples.py

hadoop jar /usr/lib/hadoop-mapreduce/hadoop-streaming.jar -files annotate_coreferences.py,dbpedia.py,file_util.py,get_training_samples.py,json_cache.py,parse_xmls_to_protos.py,paths.py,relations.py,sentence_pb2.py,sparql_client.py,training_samples_pb2.py,wikidata_util.py,mr_generate_training_samples.py -input /user/prvak/wiki-processed-sub-0.001 -output /user/prvak/wiki-samples-sub-0.001 -mapper mr_generate_training_samples.py

hadoop jar /usr/lib/hadoop-mapreduce/hadoop-streaming.jar -files annotate_coreferences.py,dbpedia.py,file_util.py,get_training_samples.py,json_cache.py,parse_xmls_to_protos.py,paths.py,relations.py,bazel-bin/mr_generate_training_samples.runfiles/__main__/sentence_pb2.py,sparql_client.py,bazel-bin/mr_generate_training_samples.runfiles/__main__/training_samples_pb2.py,wikidata_util.py,mr_generate_training_samples.py -input /user/prvak/wiki-processed-sub-0.001 -output /user/prvak/wiki-samples-sub-0.001 -mapper mr_generate_training_samples.py

hadoop jar /usr/lib/hadoop-mapreduce/hadoop-streaming.jar -input /user/prvak/wiki-processed-sub-0.001 -output /user/prvak/wiki-samples-sub-0.001 -mapper mr_generate_training_samples.py -file annotate_coreferences.py -file dbpedia.py -file file_util.py -file get_training_samples.py -file json_cache.py -file parse_xmls_to_protos.py -file paths.py -file relations.py -file sentence_pb2.py -file sparql_client.py -file training_samples_pb2.py -file wikidata_util.py -file mr_generate_training_samples.py

chmod +x hadoop jar /usr/lib/hadoop-mapreduce/hadoop-streaming.jar -input /user/prvak/wiki-processed-sub-0.001 -output /user/prvak/wiki-samples-sub-0.001 -mapper mr_generate_training_samples.py -file annotate_coreferences.py -file dbpedia.py -file file_util.py -file get_training_samples.py -file json_cache.py -file parse_xmls_to_protos.py -file paths.py -file relations.py -file bazel-genfiles/sentence_pb2.py -file sparql_client.py -file bazel-genfiles/training_samples_pb2.py -file wikidata_util.py -file mr_generate_training_samples.py
