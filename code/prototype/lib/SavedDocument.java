import org.json.*;
import org.apache.hadoop.hbase.util.Bytes;
import org.apache.hadoop.hbase.client.Get;
import org.apache.hadoop.hbase.client.Result;

public class SavedDocument {
	public String title;
	public String plaintext;
	public String corenlpXml;
	public JSONObject spotlightJson;

	// Generated by add_join.
	// private TODO sentences;
	// private TODO coreferences;
	// private TODO spotlight_mentions;

	private static byte[] WIKI = Bytes.toBytes("wiki");
	private static byte[] PLAINTEXT = Bytes.toBytes("plaintext");
	private static byte[] CORENLP_XML = Bytes.toBytes("corenlp_xml");
	private static byte[] SPOTLIGHT_JSON = Bytes.toBytes("spotlight_json");

	public static Get getGet(String title) {
		Get get = new Get(Bytes.toBytes(title));
		get.addColumn(WIKI, PLAINTEXT);
		get.addColumn(WIKI, CORENLP_XML);
		get.addColumn(WIKI, SPOTLIGHT_JSON);
		return get;
	}

	public SavedDocument(Result result) {
//		if (result.getRow() == null) {
//			throw new InvalidArgumentException("Null rowkey");
//		}

		title = new String(result.getRow());

		byte[] plaintextBytes = result.getValue(WIKI, PLAINTEXT);
		if (plaintextBytes != null) {
			plaintext = new String(plaintextBytes);
		} else {
			plaintext = null;
		}

		byte[] corenlpXmlBytes = result.getValue(WIKI, CORENLP_XML);
		if (corenlpXmlBytes != null) {
			corenlpXml = new String(corenlpXmlBytes);
		} else {
			corenlpXml = null;
		}
		byte[] spotlightJsonBytes = result.getValue(WIKI, SPOTLIGHT_JSON);
		if (spotlightJsonBytes != null) {
			spotlightJson = new JSONObject(new String(spotlightJsonBytes));
		} else {
			spotlightJson = null;
		}
	}

	public JSONObject toJSON() {
		JSONObject json = new JSONObject();
		json.put("title", title);
		json.put("plaintext", plaintext);
		json.put("corenlp_xml", corenlpXml);
		json.put("spotlight_json", spotlightJson);
		return json;
	}
}
