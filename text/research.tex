\chapter{Research}

Knowledge base completion (KBC) is the task of adding missing facts to an
incomplete knowledge base.

Knowledge base completion by relation extraction alone cannot use rich structure
of knowledge base.

\section{Predicting new edges by inference}

By automated reasoning over an existing knowledge graph, we can predict the
existence of new edges. For instance, when a person was born in Germany, it's
likely the person's nationality is German.

Both PRA and SFE produce a feature matrix -- for each node pair and relation
type, they give a vector of features. The vector is then fed to a binary
classifier, e.g., logistic regression.

\subsection{Path ranking algorithm (PRA)}
\label{path-ranking-algorithm}

The path ranking algorithm\cite{path-ranking-algorithm}
tries to predict the existence of unseen instances of a relation.
First, in a training phrase, it collects known pairs of nodes in that relation.
For each such A-B pair, it performs random walks starting from A along KG edges
and keeps the paths that reach the entity B. Each path is converted into a
\textit{path prototype}, which is a list of edge types taken along the path, e.g.
"place-of-birth, people-born-here, education, institution" could connect a
person to her college.

After the training phrase, when we try to predict the probability of a relation
between entities P and Q, for each interesting path prototype, we compute the
probability of reaching Q from P by performing a random walk among paths
following the same path prototype.
The probabilities are then used as the feature vector for a binary classifier.

TODO: Fakt to je pravdepodobnost uvnitr tohohle prototypu? Neni to uvnitr vsech
cest?

The path prototypes we use as features are selected either as the top $K$ path
prototypes by frequency among training pairs (TODO: Gardner et al.), or by
measures of precision and recall (TODO: Lao et al.)

PRA has a strong connection to logical inference. (TODO: Investigate --
combining-vector-space-embeddings-with-inference.)
The feature space extracted by PRA is a restricted class of Horn clauses over
the graph.\cite{subgraph-feature-extraction}

The prediction step is very expensive: calculating the random walk
probabilities exactly by BFS takes time $\O(d^l)$, where $d$ is the
average out-degree of the graph and $l$ is the length of path prototypes,
\textit{per entity pair}.
The exponent can be cut in half by using a two-sided BFS with some careful
bookkeeping. We can also estimate the probability by using random walks with
rejection sampling.

TODO: Neat list of embedding models in kb-completion-using-subgraph-features

TODO: Look at some relevant article, get numbers.

\subsection{Subgraph Feature Extraction (SFE)}

TODO: This is something we should try.

Subgraph Feature Extraction\cite{subgraph-feature-extraction} is a more
efficient algorithm than PRE and it allows the binary classifier to use richer
features. On a KB completion task on NELL, it improved precision from .432 with
PRE to .528.

% Code: http://rtw.ml.cmu.edu/emnlp2015\_sfe/

The authors of the algorithm observed that the probabilities expensively
computed by PRA at the prediction step are not
useful\cite{subgraph-feature-extraction} -- just setting a binary flag for the
presence or absence of a path gives statistically indistinguishable performance.
SFE is also faster by an order of magnitude.

TODO: what's K?

Given a pair of nodes, SFE runs $k$ one-sided random walks starting from both,
and then joins the subgraphs induced by these random walks on common ends of
random walks. Features are then extracted from this subgraph.
The subgraph extraction step can be augmented to include a few steps of BFS
that includes relation types with a reasonably low fan-out.
(PRA leverages path-constrained random walks, this lets us have the same
benefit without having to include lots of irrelevant entities reachable via,
e.g., "entity-has-type" edges.)

The simplest features extracted from the common subgraph are equivalent to
binarized PRA features: for each path connecting the two starting nodes, a
feature representing the existence of a path of that type is added.
Additionally, SFE makes possible the extraction of certain more expressive
features that are not encodable by PRA, or hard to sample with rejection
sampling (used in one variant of PRA). These features significantly improve
performance.

The added features include:
\begin{itemize}
\item Path bigrams (one feature for every pair of adjacent
edge types, plus begin-end marker edges).
\item One-sided features (e.g., "SOURCE:-GENDER-:male", "TARGET:-GENDER-:female"
are good predictors of marriage; capitals are likely to have many sports teams)
\item Comparisons of one-sided features. (Where constructing PRA features
"intersect the graphs on common ends of walks", comparisons "intersect on
path types".) "COMPARISON:-Gender-:/m/Male:/m/Female"
\item Vector-similar features. Find semantically similar features by an
embedding technique, then for every edge type on every path type, generate
a vector-similar feature by replacing the edge with a similar edge type.
(Replace just 1 edge on each path to avoid combinatorial explosion.)
(Also including a vector-similar feature that doesn't change any edge.)
\item ANYREL features: similarly to vector-similar features, add features
by replacing each single edge by a "any relation" marker. (Sometimes, the
existence of any relation between entities is useful evidence.)
\end{itemize}

TODO: Graph-based methods for KB completion
TODO: Neelakantan et al. (2015)

Low probability of hitting, e.g. "city in state" if node has high fan-out =>
do a few steps of BFS on promising types. (And exclude edge types of too high
fan-out -- e.g. /type/object/type). (SFE-BFS vs. SFE-RW)

TODO: "On Obtaining Negative Evidence"

TODO: How long are the random paths?

TODO: Textual relations -- high-weighted

\subsection{Embedding}

TODO

\section{Knowledge Vault}

\textit{Knowledge Vault}\cite{knowledge-vault} is Google's project of extending its proprietary
Knowledge Graph with relations extracted by reasoning over the knowledge graph
and from a corpus of plain text, HTML pages and HTML tables. Because
the sources are noisy, each relation has an associated confidence score
(an estimate of the probability that the relation actually holds).

Knowledge Vault has 1.6B triples, of which 324M have a confidence of 0.7 or
higher, and 271M have a confidence of 0.9 or higher.
This is 38x more than previous comparable system - DeepDive.
About 1/3 of the 271M confident triples were not previously in Freebase, so
the noisy sources are actually adding new relations.

Knowledge Vault consists of several \textit{extractors}. Each extractor runs on a
document and yields triples and confidence scores, representing uncertainty
about the identity of the relation and its arguments. The confidence score is
the probability of the relation being true, conditional on features seen by
the extractor.
For each possible triple, Knowledge Vault also learns its \textit{prior
probability}, conditional only on information present in the source knowledge
graph.
The final confidence scores are calculated by \textit{knowledge fusion},
which combines extractor outputs with prior probabilities.
Confidence scores from extractors are calibrated to fit the actual probabilities
with Platt scaling.
(An alternative would be using isotonic regression.)

Priors are probabilities of a relation being true, conditional
on the prior knowledge graph. They are useful for checking the consistency of
facts extracted from unreliable sources with known true facts in the knowledge
graph.

Evaluation protocol: 1) extract 1.6B candidates, 2) split them into test/train

TODO: Why are there fewer confident facts in Knowledge Vault than in Freebase?
Confident facts:

Freebase: 1500 entity types, 40M instances, 35000 relation types, 637M
facts.
Knowledge Graph: 1500 entity types, 570M instances, 35000 relation types, 18000M
facts.
Knowledge Vault: 1100 entity types, 45M instances, 4469 relation types, 271M
facts.

To prepare training data including negative samples, Knowledge Vault uses the
\textit{local closed world assumption}: e.g., if the set of actors in a movie listed
in Knowledge Graph is non-empty, it is assumed to be complete and all other
actors are used as negative samples.
They tested the assumption and LCWA labels are not too far off.

\paragraph{Extractors}
The text extractor performs entity recognition, POS tagging, dependency parsing,
co-reference resolution (within one document) and entity linkage (from mentions
to KG entities).
Then, a binary classifier (using logistic regression) is trained for each of
4469 relation types using distant supervision.
For the HTML extractor, similar features are used, augmented with DOM
information.
The remaining two extractors find relations in HTML tables and in webpages
with microformats or schema.org metadata.

For each relation, the output of all extractors is fused into one probability
conditional on the features used by all extractors. This is performed by
logistic regression, whose inputs are confidence scores of each extractor and
the number of sources used by each extractor, represented as its square root.
The classifier can learn the relative reliabilities of each extractor.
They tried logistic regression, and got a better result with boosted decision
stumps.

TODO: RESULTS of Knowledge Vault

Two algorithms were used to predict prior probabilities.
One of them is PRA \ref{path-ranking-algorithm}.

\subsection{Neural network models}

\paragraph{Simple model}
Low-rank decomposition of 3d $E\times P\times E$ matrix. Learn $K$-dimensional
latent vectors, two for each entity ($u_s$ and $v_o$) and one for each relation
($w_p$). $K=60$. (Equivalent to PARAFAC method of tensor decomposition +
sigmoid.)
$$\Pr[G(s,p,o)=1] = \sigma(\sum\limits{k=1}^K u_{sk} w_{pk} v_{ok})$$

\paragraph{[37]}
Associate a different tensor with each relation.
$$\Pr[G(s,p,o)=1] = \sigma(\vec{\beta}_p f[\vec{u}_s^T \vec{W}_p^{1:M} \vec{v}_o])$$
$\vec{\beta}_p\in\R^K$, $\vec{W}_p^m\in\R^{K\times K}$.
Number of parameters: $\O(KE+K^2 MP)$ ($M$: number of "layers" in $W_p$ tensors)

\paragraph{The one they actually used}
One vector per predicate, then MLP to capture interaction terms.
$$\Pr[G(s,p,o)=1]=\sigma(\vec{\beta}^T f[\vec{A} [\vec{u}_s; \vec{w}_p, \vec{v}_o] ])$$
$\vec{A}\in\R^{L\times 3K}$.
$L$ second-layer weights in $\vec{\beta}$.
Parameters: $\O(L+LK+KE+KP)$, essentially same performance as [37].

Neural network models have essentially same AUC as PRA
(0.882 for NN, 0.884 for PRA).
The outputs of both systems for estimating the prior probability are then
fused. An extra feature is a 0/1 flag specifying whether the given prior system
could make a prediction.

Finally, the priors and the extracted confidence scores are again fused.

\paragraph{Proposed extensions}
Modeling mutual exclusion, modeling soft correlations
(e.g., most people have <=10 kids), multiple levels of abstraction (born in
Honolulu vs. born in Hawaii), discounting correlated sources, temporarily true
facts, adding new entities and relation types.

\section{Boosted decision stumps}
A decision stump is a one-level decision tree, a type of weak learner
well suited for boosting (for instance, by AdaBoost).
