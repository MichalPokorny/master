\chapter{Research}

TODO: How do ontologies look?

The Linked Data cloud contains roughly 1000 datasets. The majority of links
connect identical entities between datasets.\cite{kg-refinement-survey}.

\section{Knowledge bases}

TODO

Many knowledge bases are often construced either from semi-structured knowledge
(e.g., Wikipedia), or harvested by large-scale NLP. Others are curated.

Curated: Cyc

Community-edited: Freebase, Wikidata

Extracted from large-scale semi-structured knowledge bases (e.g. Wikipedia):
DBpedia, YAGO

Information extraction from unstructured or semi-structured data: NELL,
PROSPERA, Knowledge Vault

\subsection{DBpedia}

DBpedia is automatically extracted from the text of Wikipedia articles.

DBpedia is created separately for every language edition of Wikipedia.
A new edition is published every year.

TODO

\subsection{Wikidata}

TODO

Wikidata was developed by Wikimedia Deutschland in 2012 and is operated by the
Wikimedia Foundation.

Wikidata's data model contains items and statements. An item represents an
entity, has an identifier called a "qid", and may have labels, descriptions and
aliases in multiple languages, and links to other Wikimedia projects. Wikidata
states \textit{do not aim to encode true facts}, but \textit{claims} from
different sources, which can also contradict each other (e.g., border
conflicts).

A statement is composed of a claim and zero or more references to the claim.

As of September 2015, Wikidata counted about 70 million statements on 18 million
entities.

Wikidata has no explicit notion of types. Type is just another relation
("instance of" -- P31).

TODO: more in-depth comparison: A Comparative Survey of DBpedia, Freebase,
OpenCyc, Wikidata and YAGO

It's a superset of Freebase.

\subsection{Freebase}

TODO

Freebase was a community-edited knowledge base originally launched by Metaweb in
2007 and acquired by Google in 2010. Aside from many usages outside Google,
Freebase served as the seed for the proprietary Google Knowledge
Graph\cite{kg-refinement-survey}.

Objects in Freebase are identified by their MID (e.g. /m/02mjmr). Each object
has a type.
Freebase uses \textit{complex value types} to represent N-ary relations for
$N>3$ (e.g., geographic coordinates, political positions held with a start and
an end). CVTs are objects and have MIDs. Non-CVT objects are called
\textit{topics} to distinguish them from CVTs.

The content of Freebase has been partially imported from other sources, such as
Wikipedia or MusicBrainz.

As of July 2016, it's closed.

\subsection{The migration from Freebase to Wikidata\cite{freebase-wikidata-migration}}

In 2014, Google decided to shut down Freebase and to offer the content
of Freebase to the Wikidata community.
The motivation for this migration was the perception of the Knowledge Graph team
at Google that the Wikidata community was in a better position to lead the
collective effort to collect and curate structured knowledge about the world.

Freebase was turned read-only on March 31, 2015, and at that time, it had 3
billion facts (000 000 000) and almost 50 million entities. Freebase data is
available as an N-triples dump in RDF under the CC-BY license at
https://developers.google.com/freebase/data.

One challenge of the migration was the difference in licensing: Wikidata is
published under CC0, Freebase is published under CC-BY. The Freebase dump had to
be filtered down to data Google could relicense under CC0. This reduced the
number of Freebase facts by about 1.4\% (by about 42 million from the original
3 billion).

Knowledge Vault\ref{knowledge-vault} was reused to provide external references
for Freebase statements without sources. A domain blacklist was used to filter
down references that would not be accepted for inclusion in Wikidata (e.g.,
social networks, reviews on shopping sites, etc.).

The data quality of imported data was found to be low by Wikidata standards.
The migration relied on the so-called \textit{Primary Sources Tool}, which
displays Freebase statements for curation by the contributor that can be added
to the currently shown Wikidata item.

Wholesale ingestion of Freebase would necessiate a large increase in
maintenance effort on part of Wikidata. To counter the costs, new tools
(including the Primary Sources Tool) were developed to help the community
more efficiently maintain the data.

Only 4.56 million Freebase entities were mapped to Wikidata items, with the help
of existing Wikipedia mappings by Google and Samsung and IDs in external
databases.
Mapped topics had an average of 13.9 facts, whereas not mapped topics had an
average of 5.7 facts. The majority of topics with at least 47 facts were maped
(191K non-mapped vs. 195K mapped), so the most important topics in Freebase were
mapped.

Around 360 properties were quickly mapped by the help of the community. In
summer of 2015 when the work was completed, Wikidata had about 1800 properties,
whereas Freebase had around 37700.

CVTs were transformed into more complex Wikidata statements:
Obama - "position held", from 20.1.2009 to (unknown value).

Factoid: removed dates prior to 1920, because cannot be sure if they're in
Gregorian or Julian calendar.

The last Freebase dump from March 2015 contained 48 million topics, 2997 million
triples, 442 million "useful" facts (excluding names, descriptions etc.) and 68
million labels.
In comparison, by August 2015, Wikidata contained 14.5 million items, 66 million
statements and 82 million labels.

Why are the numbers so different? Freebase has a lot of topics that do not match
Wikidata's notability criteria (https://www.wikidata.org/wiki/Wikidata:Notability).
Freebase is also much more redundant than Wikidata -- properties often have a
\textit{reverse}.
CVTs use a lot of facts that are represented as one Wikidata statement.

If we attempted to encode Wikidata statements as if they were Freebase facts
(remove sources, use CVTs, add reverses) -> would lead to 110 million facts
(167\% increase over raw number of statements).
Freebase also often contains duplicate data.

The migration added about 14 million new statements, which would increase the
number of Wikidata statements by ~21\% if they were all added.

Why is this so low compared to 3 billion facts? Because only ~4.56 million items
were mapped to Wikidata.

Restricted to human-reviewed facts (1.6M), of them, 0.58M have their subject
mapped to Wikidata, of which 0.52M (92\%) are converted to Wikidata statements.
Of those, 58\% are already in Wikidata, so they added 0.25M new reviewed
statements to Wikidata.

Raw number of triples is NOT a good measure of quality.

[17] in The Great Migration discusses Wikidata representation in RDF.

\subsection{YAGO}

TODO

\subsection{NELL}

TODO

\section{Knowledge base completion}

Knowledge base completion (KBC) is the task of adding missing facts to an
incomplete knowledge base.

Knowledge base completion by relation extraction alone cannot use rich structure
of knowledge base.

\section{Inference on graph features}

By automated reasoning over an existing knowledge graph, we can predict the
existence of new edges. For instance, when a person was born in Germany, it's
likely the person's nationality is German.

Both PRA and SFE produce a feature matrix -- for each node pair and relation
type, they give a vector of features. The vector is then fed to a binary
classifier, e.g., logistic regression.

\subsection{Path ranking algorithm (PRA)}
\label{path-ranking-algorithm}

The path ranking algorithm\cite{path-ranking-algorithm}
tries to predict the existence of unseen instances of a relation.
First, in a training phrase, it collects known pairs of nodes in that relation.
For each such A-B pair, it performs random walks starting from A along KG edges
and keeps the paths that reach the entity B. Each path is converted into a
\textit{path prototype}, which is a list of edge types taken along the path, e.g.
"place-of-birth, people-born-here, education, institution" could connect a
person to her college.

After the training phrase, when we try to predict the probability of a relation
between entities P and Q, for each interesting path prototype, we compute the
probability of reaching Q from P by performing a random walk among paths
following the same path prototype.
The probabilities are then used as the feature vector for a binary classifier.

TODO: Fakt to je pravdepodobnost uvnitr tohohle prototypu? Neni to uvnitr vsech
cest?

The path prototypes we use as features are selected either as the top $K$ path
prototypes by frequency among training pairs (TODO: Gardner et al.), or by
measures of precision and recall (TODO: Lao et al.)

PRA has a strong connection to logical inference. (TODO: Investigate --
combining-vector-space-embeddings-with-inference.)
The feature space extracted by PRA is a restricted class of Horn clauses over
the graph.\cite{subgraph-feature-extraction}

The prediction step is very expensive: calculating the random walk
probabilities exactly by BFS takes time $\O(d^l)$, where $d$ is the
average out-degree of the graph and $l$ is the length of path prototypes,
\textit{per entity pair}.
The exponent can be cut in half by using a two-sided BFS with some careful
bookkeeping. We can also estimate the probability by using random walks with
rejection sampling.

TODO: Neat list of embedding models in kb-completion-using-subgraph-features

TODO: Look at some relevant article, get numbers.

\subsection{Subgraph Feature Extraction (SFE)}

TODO: This is something we should try.

Subgraph Feature Extraction\cite{subgraph-feature-extraction} is a more
efficient algorithm than PRE and it allows the binary classifier to use richer
features. On a KB completion task on NELL, it improved precision from .432 with
PRE to .528.

% Code: http://rtw.ml.cmu.edu/emnlp2015\_sfe/

The authors of the algorithm observed that the probabilities expensively
computed by PRA at the prediction step are not
useful\cite{subgraph-feature-extraction} -- just setting a binary flag for the
presence or absence of a path gives statistically indistinguishable performance.
SFE is also faster by an order of magnitude.

TODO: what's K?

Given a pair of nodes, SFE runs $k$ one-sided random walks starting from both,
and then joins the subgraphs induced by these random walks on common ends of
random walks. Features are then extracted from this subgraph.
The subgraph extraction step can be augmented to include a few steps of BFS
that includes relation types with a reasonably low fan-out.
(PRA leverages path-constrained random walks, this lets us have the same
benefit without having to include lots of irrelevant entities reachable via,
e.g., "entity-has-type" edges.)

The simplest features extracted from the common subgraph are equivalent to
binarized PRA features: for each path connecting the two starting nodes, a
feature representing the existence of a path of that type is added.
Additionally, SFE makes possible the extraction of certain more expressive
features that are not encodable by PRA, or hard to sample with rejection
sampling (used in one variant of PRA). These features significantly improve
performance.

The added features include:
\begin{itemize}
\item Path bigrams (one feature for every pair of adjacent
edge types, plus begin-end marker edges).
\item One-sided features (e.g., "SOURCE:-GENDER-:male", "TARGET:-GENDER-:female"
are good predictors of marriage; capitals are likely to have many sports teams)
\item Comparisons of one-sided features. (Where constructing PRA features
"intersect the graphs on common ends of walks", comparisons "intersect on
path types".) "COMPARISON:-Gender-:/m/Male:/m/Female"
\item Vector-similar features. Find semantically similar features by an
embedding technique, then for every edge type on every path type, generate
a vector-similar feature by replacing the edge with a similar edge type.
(Replace just 1 edge on each path to avoid combinatorial explosion.)
(Also including a vector-similar feature that doesn't change any edge.)
\item ANYREL features: similarly to vector-similar features, add features
by replacing each single edge by a "any relation" marker. (Sometimes, the
existence of any relation between entities is useful evidence.)
\end{itemize}

TODO: Graph-based methods for KB completion
TODO: Neelakantan et al. (2015)

Low probability of hitting, e.g. "city in state" if node has high fan-out =>
do a few steps of BFS on promising types. (And exclude edge types of too high
fan-out -- e.g. /type/object/type). (SFE-BFS vs. SFE-RW)

TODO: "On Obtaining Negative Evidence"

TODO: How long are the random paths?

TODO: Textual relations -- high-weighted

\section{Embedding methods for KBC}

Learns embedded representation of entities and relations, and infers missing
relationships.

TODO

\section{Knowledge Vault}

\textit{Knowledge Vault}\cite{knowledge-vault} is Google's project of extending
its proprietary Knowledge Graph with relations extracted by reasoning over
the knowledge graph and from a corpus of plain text, HTML pages and HTML
tables. Because the sources are noisy, each relation has an associated
confidence score (an estimate of the probability that the relation actually
holds).

Knowledge Vault has 1.6B triples, of which 324M have a confidence of 0.7 or
higher, and 271M have a confidence of 0.9 or higher.
This is 38x more than previous comparable system - DeepDive.
About 1/3 of the 271M confident triples were not previously in Freebase, so
the noisy sources are actually adding new relations.

Knowledge Vault consists of several \textit{extractors}. Each extractor runs on a
document and yields triples and confidence scores, representing uncertainty
about the identity of the relation and its arguments. The confidence score is
the probability of the relation being true, conditional on features seen by
the extractor.
For each possible triple, Knowledge Vault also learns its \textit{prior
probability}, conditional only on information present in the source knowledge
graph.
The final confidence scores are calculated by \textit{knowledge fusion},
which combines extractor outputs with prior probabilities.
Confidence scores from extractors are calibrated to fit the actual probabilities
with Platt scaling\ref{platt-scaling}.
(An alternative would be using isotonic regression.)

Priors are probabilities of a relation being true, conditional
on the prior knowledge graph. They are useful for checking the consistency of
facts extracted from unreliable sources with known true facts in the knowledge
graph.

Evaluation protocol: 1) extract 1.6B candidates, 2) split them into test/train

TODO: Why are there fewer confident facts in Knowledge Vault than in Freebase?
Confident facts:

Freebase: 1500 entity types, 40M instances, 35000 relation types, 637M
facts.
Knowledge Graph: 1500 entity types, 570M instances, 35000 relation types, 18000M
facts.
Knowledge Vault: 1100 entity types, 45M instances, 4469 relation types, 271M
facts.

To prepare training data including negative samples, Knowledge Vault uses the
\textit{local closed world assumption}: e.g., if the set of actors in a movie listed
in Knowledge Graph is non-empty, it is assumed to be complete and all other
actors are used as negative samples.
They tested the assumption and LCWA labels are not too far off.

\paragraph{Extractors}
The text extractor performs entity recognition, POS tagging, dependency parsing,
co-reference resolution (within one document) and entity linkage (from mentions
to KG entities).
Then, a binary classifier (using logistic regression) is trained for each of
4469 relation types using distant supervision.
For the HTML extractor, similar features are used, augmented with DOM
information.
The remaining two extractors find relations in HTML tables and in webpages
with microformats or schema.org metadata.

For each relation, the output of all extractors is fused into one probability
conditional on the features used by all extractors. This is performed by
logistic regression, whose inputs are confidence scores of each extractor and
the number of sources used by each extractor, represented as its square root.
The classifier can learn the relative reliabilities of each extractor.
They tried logistic regression, and got a better result with boosted decision
stumps\ref{boosted-decision-stumps}.

TODO: RESULTS of Knowledge Vault

Two algorithms were used to predict prior probabilities.
One of them is PRA \ref{path-ranking-algorithm}.

\subsection{Neural network models}

\paragraph{Simple model}
Low-rank decomposition of 3d $E\times P\times E$ matrix. Learn $K$-dimensional
latent vectors, two for each entity ($u_s$ and $v_o$) and one for each relation
($w_p$). $K=60$. (Equivalent to PARAFAC method of tensor decomposition +
sigmoid.)
$$\Pr[G(s,p,o)=1] = \sigma(\sum\limits{k=1}^K u_{sk} w_{pk} v_{ok})$$

\paragraph{[37]}
Associate a different tensor with each relation.
$$\Pr[G(s,p,o)=1] = \sigma(\vec{\beta}_p f[\vec{u}_s^T \vec{W}_p^{1:M} \vec{v}_o])$$
$\vec{\beta}_p\in\R^K$, $\vec{W}_p^m\in\R^{K\times K}$.
Number of parameters: $\O(KE+K^2 MP)$ ($M$: number of "layers" in $W_p$ tensors)

\paragraph{The one they actually used}
One vector per predicate, then MLP to capture interaction terms.
$$\Pr[G(s,p,o)=1]=\sigma(\vec{\beta}^T f[\vec{A} [\vec{u}_s; \vec{w}_p, \vec{v}_o] ])$$
$\vec{A}\in\R^{L\times 3K}$.
$L$ second-layer weights in $\vec{\beta}$.
Parameters: $\O(L+LK+KE+KP)$, essentially same performance as [37].

Neural network models have essentially same AUC as PRA
(0.882 for NN, 0.884 for PRA).
The outputs of both systems for estimating the prior probability are then
fused. An extra feature is a 0/1 flag specifying whether the given prior system
could make a prediction.

Finally, the priors and the extracted confidence scores are again fused.

\paragraph{Proposed extensions}
Modeling mutual exclusion, modeling soft correlations
(e.g., most people have <=10 kids), multiple levels of abstraction (born in
Honolulu vs. born in Hawaii), discounting correlated sources, temporarily true
facts, adding new entities and relation types.

\section{Boosted decision stumps}
\label{boosted-decision-stumps}
A decision stump is a one-level decision tree, a type of weak learner
well suited for boosting (for instance, by AdaBoost).

\section{Platt scaling}
\label{platt-scaling}

Platt scaling\cite{platt-scaling} is a technique that scales the output $x$ of a binary classifier
to the probability the sample belongs to the positive class. It is used as a
calibration step for models that do not automatically learn to give good
probability estimates.

The probability estimated by Platt scaling that the sample $x$ belongs to the
positive class is:
$$\Pr[y=1|x] = \frac{1}{1+\exp(Ax+B)}$$
$A$ and $B$ are parameters trained on a held-out calibration set.
Platt suggested learning them using Levenberg-Marquardt.

\section{Isotonic regression}
TODO

\section{Knowledge graph refinement survey\cite{kg-refinement-survey}}

Most existing work for KG refinement runs on DBpedia, YAGO, or Freebase.

It's useful to study refinement isolated from construction -- better
understanding and separation of effects, generality over KGs.

The authors classify KG refinement methods as \textit{completion}, or
\textit{error correction}. Completion enhances completeness, correction
reduces error.
Some approaches are also \textit{targeted} to correct only some part of the
knowledge graph, e.g. one relation type (like entity types), knowledge graph
intelinks or literal values.
Some methods try to extends the knowledge graph's \textit{schema}.
\textit{Internal} methods only operate on the input knowledge graph, while
\textit{external} methods use additional data, like text corpora.

There are several approaches to evalute the methods.
\paragraph{KG as silver standard}
KG completion can use the KG itself as a \textit{silver standard}: we check how
well can we predict existing relations in the KG if we hide them from the
algorithm. Silver standard is not suitable to measure error detection, because
it assumes the KG to be correct.
A slight problem is that the entire KG is used as input to completion and then
also for evaluation, but it would be hard to split it into a "train" and "test"
set, so most work uses the entire KG anyway.

The measured metrics are usually recall, precision and F-measure ($F_1 =
2\cdot\frac{prec \cdot recall}{prec + recall}$).

Another problem is that most KGs are \textit{open world}, and if the method
correctly completes a new edge that's missing in the knowledge graph, it would
count as a negative sample and lower the precision. To obtain better negative
samples, we can use the \textit{local closed world assumption}.

\paragraph{Partial gold standard}
We can also use a \textit{partial gold standard}: take a part of the KG and
manually fill in every relation that "should be there" (for completion), or
manually label a set of relations as correct or incorrect (for correction).
Completion is usually again compared by recall, precision and $F_1$.
Correction uses accuracy and/or area under ROC curve (AUC).
Gold standards can be collected from human experts, or we can use another
interlinked KG as a partial gold standard (through 20\% of interlinks between
DBpedia and Freebase are incorrect; ~50\% of owl:sameAs connect similar, but not
exactly same entities).

\paragraph{Ex post evaluations}
Completions or corrections are given to human raters to label as correct or
incorrect. Useful, for example, when partial gold standard from high-quality KG
is used to evaluate correction methods, the standard might not have
meaningfully many errors.

\subsection{KG completion methods}
Internal methods use only the KG itself as input.

\paragraph{Internal - Classification}
Supervised classification can be used to predict entity types (sometimes
\textit{multi-label} - e.g. Arnold Schwarzenegger is a Politician and Actor).
The features used for classification are usually relations connecting to other
entities. In hierarchical ontologies, type prediction is a \textit{hierarchical
classification} problem (through there have been no applications of it so far).
Classification can be used also to predict the existence of a relation between
a pair of entities.
One work predicts the existence of relations between entities by a tensor
neural network, based on chains of other
relations in Freebase and WordNet.\cite{neural-tensor-networks-for-kbc}

TODO: [76]

TODO: again. figure out what exactly's going on here.

\paragraph{Probabilistic and statistical methods}
SDType: predicts the type of an entity based on graph features (e.g., lots of
"cast" nodes mean high likelihood it's an Actor). Deployed on DBpedia, adds 3.4
million type statements.
ProSWIP: predicts entity types.

Statistical methods to enrich schema with additional domains/ranges of
relations, disjointness axioms, ...

Oren et al. [58]: predicts relation, but not which entity it connects to.

[60]: Predict missing types in DBpedia based on redundancies in type systems
(YAGO vs. DBpedia ontology -- based on infoboxes vs. categories).

Association rule mining to extend schemas (e.g., domain/range restrictions,
disjointness axioms, ...) [80, 20]

\paragraph{External methods}
External methods use data sources external to the KG (e.g., knowledge graph
interlinks, Wikipedia, text corpora, ...).

\paragraph{External - Classification}
[57]: predict entity types by k-NN on Wikipedia link graph.
There are typically more interlinks between Wikipedia pages than KG entities.
[3]: different-language DBpedias to predict missing types.

\paragraph{External - NLP-based methods}
Text-based methods for predicting types and relations.
Types: Hearst patterns[28] ("ISA") in abstracts ([23, 36]).
[38] learns patterns on Wiki abstract using conditional random fields [37].
On entire articles: [86]

Prediction of relation between two entities: usually by \textit{distant
supervision}\ref{distant-supervision}.

\paragraph{External - information extraction from semi-structured data}
For example, some methods leverage extra tags in Wikipedia.
This is usually combined with DBpedia.

[51] extract information from tables in Wikipedia.
For two entities co-occurring in a table, they likely share a relation.
They first collect all relations that could possibly hold between at least 1
pair of entities, then train a classifier to predict which relation actually
holds.

[65] proposes the use of List pages in Wikipedia to generate type and relation
assertions (e.g., "List of Jewish-American Writers").

\paragraph{Knowledge Graph Fusion based on interlinks}
E.g.: different DBpedia languages ([10]). [18]: probabilistic mapping between
KGs.

\subsection{Error detection methods}
No methods combine completion and correction. The only exception is SDType and
SDValidate (they can output both errors and completion axioms).

\paragraph{Internal - Classification}
Knowledge Vault uses binary classification to tell which relations should hold,
as a cleaning step after knowledge extraction.

\paragraph{Internal - Reasoning}
Reasoning is a semantic web technique that finds whether a set of assertions is
free of contradictions (based on a rich ontology).
NELL and PROSPERA use reasoning to determine whether a proposed new axiom is
plausible. Real KGs have to deal with errors and noise.

Many KGs - e.g., DBpedia - don't have enough richness for useful reasoning
(e.g., disjointness assertions). We need to enrich such ontologies to let
reasoning find errors -- e.g. [78] uses statistical methods; [39] association
rule mining; [46] inductive logic programming.

\paragraph{Internal - Outlier detection}
Numeric literals are a natural target. ([85]: applied to DBpedia; most found
outliers are actual errors; extra split -- e.g. population, for countries,
cities, towns in isolation)

Outlier detection can be used to find suspicious interlinks -- e.g., using
entity types in both KGs as features.
Techniques: Local Outlier Factor [9], cluster-based outlier detection [27].

[63]: characteristic distribution of subject/object types for every relation

Graph-based measures (degree, clustering coefficient, centrality, ...) to
identify wrong KG interlinks [25] -- compares expected to actual distribution
(e.g., power-law-like for degree of entities).

\paragraph{External}
External methods for error detection are few.
DeFacto[40] checks facts in DBpedia by lexicalizing them to natural language
sentences and searching the Web ofr them. Sentences with few hits are assigned
a low confidence score.

\subsection{Findings}

\paragraph{Methods}

\paragraph{No combining of error detection and completion}
Only SDType/SDValidate combines error detection and completion (the algorithms
are highly similar and output either completion axioms, or errors).
In principle, we could use many approaches to do both (e.g., find relations
which have no supporting statements in corpus), but they don't.
Boosting weights, random forest attribute weights, ... could be used to develop
a system for both.

Hardly any error-detecting mechanisms also say how to correct the error.

Most approaches also focus only on one target -- rarely on e.g. both type and
relation assertions.

\paragraph{Adding new entities}
No examined approaches try to \textit{add new} entities. Entity set expansion
methods have been deeply investigated in NLP. ([59, 72, 83])
This might be interesting to apply to long-tail entities.
Example: what if there's a missing page in Wikidata? Wikipedia has fairly high
standards for inclusion.

Many methods are not genuinely graph-based. For example, we could do graph-based
outlier detection or association rule mining ([2, 35]) or graph kernel functions
(e.g. for SVMs) for RDF graphs ([33, 44, 15]).

\paragraph{Evaluation methodologies}

Most papers use precision and recall (or, post-hoc, precision and number of
extracted statements). Others: ROC curves, accuracy, root mean squared error,
...
KG as silver standard, ex-post evaluations and partial gold standards appear
with equal frequency. Ex-post evaluations are mostly used for error detection.

Partial gold standards are usually publicly available, which allows for
replication and comparison.

The major KG used is DBpedia. One problem is that DBpedia is republished every
year.

Roughly 2/3 of approaches evaluated on DBpedia are evaluated \textit{only} on
DBpedia. Roughly 1/2 of approaches are evaluated on only one knowledge graph.
Some work only on DBpedia \textit{by design}.

The question of computational performance and scalability is often neglected.

It would be useful to have a common selection of benchmarks, as in, e.g.,
question answering.

There are few holistic solutions which try to improve the entire KG at once.

\section{Distant supervision}
\label{distant-supervision}
Distant supervision\cite{distant-supervision} predicts the existence of
relations based on positive and negative samples of relations and a large text
corpus. First, Named Entity Recognition is performed on the corpus to find KG
entities.
Then, Reference Resolution resolves references within each document.

Distant supervision is trained to predict the occurence of one relation,
e.g. IsCapitalOf.

If entities X and Y are in the IsCapitalOf, each sentence that contains both
entity X and entity Y is presumed to potentially contain information about
the IsCapitalOf relation existing between X and Y.
For all positive and negative samples, we find all sentences containing the two
entities, transform the sentences (and the entity mentions) into a feature
vector and train a binary classifier.
The binary classifier is then run on the entire corpus to predict unknown
relations.

The negative samples can be selected
by randomly sampling a pair of entities with the proper type that don't have the
IsCapitalOf relation. We assume that this is a true negative, not a missing
relation in the knowledge graph.

The features fed into the binary classifier include:
\begin{itemize}
\item The sequence of words between the two entities.
\item The POS tags of this sequence.
\item A flag indicating which entity came first.
\item A window of $k$ words to the left of entity 1 and right of entity 2.
\item The dependency path between the two entities in the parse.
\item For each entity, one "window" node not part of the dependency path.
	(A node connected to one of the two entities and not part of the path.)
		-- generate 1 conjunctive feature for each pair of left, right
		window nodes, and features which omit one or both of them
	\cite{distant-supervision}
\item Conjunction features.
\end{itemize}

[50]: for Freebase
[4]: for DBpedia
in both cases: Wikipedia is the corpus

TODO: kb-completion-via-search-based-qa
