\chapter{Research}

TODO: How do ontologies look?

The Linked Data cloud contains roughly 1000 datasets. The majority of links
connect identical entities between datasets.\cite{kg-refinement-survey}.

TODO: Linked Data
TODO: Semantic web

\section{Knowledge bases}

TODO

Many knowledge bases are often construced either from semi-structured knowledge
(e.g., Wikipedia), or harvested by large-scale NLP. Others are curated.

Curated: Cyc

Community-edited: Freebase, Wikidata

Extracted from large-scale semi-structured knowledge bases (e.g. Wikipedia):
DBpedia, YAGO

Information extraction from unstructured or semi-structured data: NELL,
PROSPERA, Knowledge Vault

\paragraph{Closed world assumption}: non-existing triples indicate false
relationships.

\paragraph{Open world assumption}: non-existing triples are of unknown status.
More justified as KGs are known to be highly incomplete. For example, 71\% of
people in Freebase have no place of birth.

Open IE systems are schemaless and represent relations between \textit{surface
names} without disambiguation: e.g., (Obama, born in, Hawaii), (Barack Obama,
place of birth, Honolulu).

\subsection{DBpedia}

DBpedia is automatically extracted from unstructured text of Wikipedia articles
and from infoboxes, which have a standardized structured format. The process of
developing and improving mappings is open to public contributions.
DBpedia maintains an ontology downloadable inthe OWL format.
DBpedia is created separately for every language edition of Wikipedia.
A new edition is published every year.

It contains 4.58M entities, of which 4.22M are in a consistent ontology:
1.4M persons, 700k places, 100k music albums, 87k films, 19k video games, 241k
organizations, 251k species, 6k diseases.
The dataset features labels and abstracts for these entities in up to 125
languages, 25.2M links to images and 29.8M links to external web pages,
50M links to other RDF datasets, 80.9M links to Wikipedia categories and 41.2M
YAGO2 categories.
DBpedia contains 3 billion RDF triples, of which 580M come from the English
Wikipedia and 2.46 billion from others.

There are 45M interlinks between DBpedia and external datasets, including
Freebase, OpenCyc, UMBEL, GeoNames, MusicBrainz, CIA World Fact Book, DBLP,
Project Gutenerg, DBtune Jamendo, Eurostat, UniProt, Bio2RDF, and US Census
data.

\paragraph{DBpedia Spotlight}
DBpedia Spotlight is a tool that locates DBpedia entities in text, including
resolution (disambiguation).

TODO

TODO: Describe extractors

TODO: Airpedia: extending class coverage of DBpedia

\subsection{Wikidata}

TODO

Wikidata was developed by Wikimedia Deutschland in 2012 and is operated by the
Wikimedia Foundation.

Wikidata's data model contains items and statements. An item represents an
entity, has an identifier called a "qid", and may have labels, descriptions and
aliases in multiple languages, and links to other Wikimedia projects. Wikidata
states \textit{do not aim to encode true facts}, but \textit{claims} from
different sources, which can also contradict each other (e.g., border
conflicts).

A statement is composed of a claim and zero or more references to the claim.

As of September 2015, Wikidata counted about 70 million statements on 18 million
entities.

Wikidata has no explicit notion of types. Type is just another relation
("instance of" -- P31).

TODO: more in-depth comparison: A Comparative Survey of DBpedia, Freebase,
OpenCyc, Wikidata and YAGO

\subsection{Commonsense reasoning KBs}

\paragraph{Cyc} For commonsense reasoning, maintained by CyCorp.
Open version: OpenCyc.

Open Mind Common Sense (OMCS), by MIT Media Lab.

\subsection{Freebase}

TODO

Freebase was a community-edited knowledge base originally launched by Metaweb in
2007 and acquired by Google in 2010. Aside from many usages outside Google,
Freebase served as the seed for the proprietary Google Knowledge
Graph\cite{kg-refinement-survey}.

Objects in Freebase are identified by their MID (e.g. /m/02mjmr). Each object
has a type.
Freebase uses \textit{complex value types} to represent N-ary relations for
$N>3$ (e.g., geographic coordinates, political positions held with a start and
an end). CVTs are objects and have MIDs. Non-CVT objects are called
\textit{topics} to distinguish them from CVTs.

The content of Freebase has been partially imported from other sources, such as
Wikipedia or MusicBrainz.

As of July 2016, it's closed.

\subsection{The migration from Freebase to Wikidata\cite{freebase-wikidata-migration}}

In 2014, Google decided to shut down Freebase and to offer the content
of Freebase to the Wikidata community.
The motivation for this migration was the perception of the Knowledge Graph team
at Google that the Wikidata community was in a better position to lead the
collective effort to collect and curate structured knowledge about the world.

Freebase was turned read-only on March 31, 2015, and at that time, it had 3
billion facts (000 000 000) and almost 50 million entities. Freebase data is
available as an N-triples dump in RDF under the CC-BY license at
https://developers.google.com/freebase/data.

One challenge of the migration was the difference in licensing: Wikidata is
published under CC0, Freebase is published under CC-BY. The Freebase dump had to
be filtered down to data Google could relicense under CC0. This reduced the
number of Freebase facts by about 1.4\% (by about 42 million from the original
3 billion).

Knowledge Vault\ref{knowledge-vault} was reused to provide external references
for Freebase statements without sources. A domain blacklist was used to filter
down references that would not be accepted for inclusion in Wikidata (e.g.,
social networks, reviews on shopping sites, etc.).

The data quality of imported data was found to be low by Wikidata standards.
The migration relied on the so-called \textit{Primary Sources Tool}, which
displays Freebase statements for curation by the contributor that can be added
to the currently shown Wikidata item.

Wholesale ingestion of Freebase would necessiate a large increase in
maintenance effort on part of Wikidata. To counter the costs, new tools
(including the Primary Sources Tool) were developed to help the community
more efficiently maintain the data.

Only 4.56 million Freebase entities were mapped to Wikidata items, with the help
of existing Wikipedia mappings by Google and Samsung and IDs in external
databases.
Mapped topics had an average of 13.9 facts, whereas not mapped topics had an
average of 5.7 facts. The majority of topics with at least 47 facts were maped
(191K non-mapped vs. 195K mapped), so the most important topics in Freebase were
mapped.

Around 360 properties were quickly mapped by the help of the community. In
summer of 2015 when the work was completed, Wikidata had about 1800 properties,
whereas Freebase had around 37700.

CVTs were transformed into more complex Wikidata statements:
Obama - "position held", from 20.1.2009 to (unknown value).

Factoid: removed dates prior to 1920, because cannot be sure if they're in
Gregorian or Julian calendar.

The last Freebase dump from March 2015 contained 48 million topics, 2997 million
triples, 442 million "useful" facts (excluding names, descriptions etc.) and 68
million labels.
In comparison, by August 2015, Wikidata contained 14.5 million items, 66 million
statements and 82 million labels.

Why are the numbers so different? Freebase has a lot of topics that do not match
Wikidata's notability criteria (https://www.wikidata.org/wiki/Wikidata:Notability).
Freebase is also much more redundant than Wikidata -- properties often have a
\textit{reverse}.
CVTs use a lot of facts that are represented as one Wikidata statement.

If we attempted to encode Wikidata statements as if they were Freebase facts
(remove sources, use CVTs, add reverses) -> would lead to 110 million facts
(167\% increase over raw number of statements).
Freebase also often contains duplicate data.

The migration added about 14 million new statements, which would increase the
number of Wikidata statements by ~21\% if they were all added.

Why is this so low compared to 3 billion facts? Because only ~4.56 million items
were mapped to Wikidata.

Restricted to human-reviewed facts (1.6M), of them, 0.58M have their subject
mapped to Wikidata, of which 0.52M (92\%) are converted to Wikidata statements.
Of those, 58\% are already in Wikidata, so they added 0.25M new reviewed
statements to Wikidata.

Raw number of triples is NOT a good measure of quality.

[17] in The Great Migration discusses Wikidata representation in RDF.

\subsection{YAGO}

YAGO (Yet Another Great Ontology)

TODO

\subsection{NELL}

NELL stands for Never-ending Language Learning.

NELL starts with an ontology with hundreds of categories (e.g., person, athlete,
sport) and binary relations (e.g., athletePlaysSport). Each category and
relation starts with a seed set of 10 to 20 positive examples.
It uses a dataset of 500 million web pages from the ClueWeb2009 corpus and
has access to 100k Google searches per day.
The objective of NELL is to extract additional beliefs from the web and to
perform this task better every day.

TODO: Carlson et al., 2010 -- Towards an architecture for never-ending language
learning (1500 different classifiers and extraction methods)

TODO

\section{Knowledge base completion}

Knowledge base completion (KBC) is the task of adding missing facts to an
incomplete knowledge base.

Knowledge base completion by relation extraction alone cannot use rich structure
of knowledge base.

\section{Inference on graph features}

By automated reasoning over an existing knowledge graph, we can predict the
existence of new edges. For instance, when a person was born in Germany, it's
likely the person's nationality is German.

Both PRA and SFE produce a feature matrix -- for each node pair and relation
type, they give a vector of features. The vector is then fed to a binary
classifier, e.g., logistic regression.

\subsection{Path ranking algorithm (PRA)}
\label{path-ranking-algorithm}

The path ranking algorithm\cite{path-ranking-algorithm}
tries to predict the existence of unseen instances of a relation.
First, in a training phrase, it collects known pairs of nodes in that relation.
For each such A-B pair, it performs random walks starting from A along KG edges
and keeps the paths that reach the entity B. Each path is converted into a
\textit{path prototype}, which is a list of edge types taken along the path, e.g.
"place-of-birth, people-born-here, education, institution" could connect a
person to her college.

After the training phrase, when we try to predict the probability of a relation
between entities P and Q, for each interesting path prototype, we compute the
probability of reaching Q from P by performing a random walk among paths
following the same path prototype.
The probabilities are then used as the feature vector for a binary classifier.

TODO: Fakt to je pravdepodobnost uvnitr tohohle prototypu? Neni to uvnitr vsech
cest?

The path prototypes we use as features are selected either as the top $K$ path
prototypes by frequency among training pairs (TODO: Gardner et al.), or by
measures of precision and recall (TODO: Lao et al.)

PRA has a strong connection to logical inference. (TODO: Investigate --
combining-vector-space-embeddings-with-inference.)
The feature space extracted by PRA is a restricted class of Horn clauses over
the graph.\cite{subgraph-feature-extraction}

The prediction step is very expensive: calculating the random walk
probabilities exactly by BFS takes time $\O(d^l)$, where $d$ is the
average out-degree of the graph and $l$ is the length of path prototypes,
\textit{per entity pair}.
The exponent can be cut in half by using a two-sided BFS with some careful
bookkeeping. We can also estimate the probability by using random walks with
rejection sampling.

TODO: Neat list of embedding models in kb-completion-using-subgraph-features

TODO: Look at some relevant article, get numbers.

Good description in \cite{random-walk-inference}.

It was shown to outperform FOIL in NELL link prediction
\cite{review-of-relational-ml-for-kgs}, comparable to ER-MLP in Knowledge Vault.
(0.884 AUC vs. 0.882 for ER-MLP)

\subsection{Subgraph Feature Extraction (SFE)}

TODO: This is something we should try.

Subgraph Feature Extraction\cite{subgraph-feature-extraction} is a more
efficient algorithm than PRE and it allows the binary classifier to use richer
features. On a KB completion task on NELL, it improved precision from .432 with
PRE to .528.

% Code: http://rtw.ml.cmu.edu/emnlp2015\_sfe/

The authors of the algorithm observed that the probabilities expensively
computed by PRA at the prediction step are not
useful\cite{subgraph-feature-extraction} -- just setting a binary flag for the
presence or absence of a path gives statistically indistinguishable performance.
SFE is also faster by an order of magnitude.

TODO: what's K?

Given a pair of nodes, SFE runs $k$ one-sided random walks starting from both,
and then joins the subgraphs induced by these random walks on common ends of
random walks. Features are then extracted from this subgraph.
The subgraph extraction step can be augmented to include a few steps of BFS
that includes relation types with a reasonably low fan-out.
(PRA leverages path-constrained random walks, this lets us have the same
benefit without having to include lots of irrelevant entities reachable via,
e.g., "entity-has-type" edges.)

The simplest features extracted from the common subgraph are equivalent to
binarized PRA features: for each path connecting the two starting nodes, a
feature representing the existence of a path of that type is added.
Additionally, SFE makes possible the extraction of certain more expressive
features that are not encodable by PRA, or hard to sample with rejection
sampling (used in one variant of PRA). These features significantly improve
performance.

The added features include:
\begin{itemize}
\item Path bigrams (one feature for every pair of adjacent
edge types, plus begin-end marker edges).
\item One-sided features (e.g., "SOURCE:-GENDER-:male", "TARGET:-GENDER-:female"
are good predictors of marriage; capitals are likely to have many sports teams)
\item Comparisons of one-sided features. (Where constructing PRA features
"intersect the graphs on common ends of walks", comparisons "intersect on
path types".) "COMPARISON:-Gender-:/m/Male:/m/Female"
\item Vector-similar features. Find semantically similar features by an
embedding technique, then for every edge type on every path type, generate
a vector-similar feature by replacing the edge with a similar edge type.
(Replace just 1 edge on each path to avoid combinatorial explosion.)
(Also including a vector-similar feature that doesn't change any edge.)
\item ANYREL features: similarly to vector-similar features, add features
by replacing each single edge by a "any relation" marker. (Sometimes, the
existence of any relation between entities is useful evidence.)
\end{itemize}

TODO: Graph-based methods for KB completion
TODO: Neelakantan et al. (2015)

Low probability of hitting, e.g. "city in state" if node has high fan-out =>
do a few steps of BFS on promising types. (And exclude edge types of too high
fan-out -- e.g. /type/object/type). (SFE-BFS vs. SFE-RW)

TODO: "On Obtaining Negative Evidence"

TODO: How long are the random paths?

TODO: Textual relations -- high-weighted

\subsection{Random walk inference and learning}

\cite{random-walk-inference} tries to draw reliable inferences from imperfectly
extracted knowledge. Logical inference is too brittle to use on automatically
extracted knowledge, probabilistic inference is too unscalable.
Evaluated on NELL.

NELL uses Horn clauses: AthletePlaysForTeam(x,y) \& TeamPlaysInLeague(y,z) =>
AtheletePlaysInLeague(x,z). There are \~600 such rules, each has a probability
that conclusion will hold.

N-FOIL: start with general rule and progressively specialize it, to cover many
positives and few negatives. Remove positives from training set.
Assumption: predicate is functional (at most 1 league per athlete).

PRA combines "expert" path types.

(A model could potentially benefit from predicting multiple relations jointly.)

Constrained PRA path generation ("required query support >=
0.01")\cite{random-walk-inference}

Low-variance sampling: to prevent that with probability 50\% two random walks
from a node with 2 relations will go the same way. To get $M$ samples, generate
$x\in[0;M^{-1}]$, then repeatedly add $M^{-1}$.

TODO: random walk with restart, aka personalized page-rank ("Topic-sensitive
pagerank")

TODO: read again

\section{Embedding methods for KBC}

Learns embedded representation of entities and relations, and infers missing
relationships.

TODO

\subsection{RESCAL}
Text from:
\cite{review-of-relational-ml-for-kgs}
Original article:
\cite{rescal}

$$f_{ijk}^{RESCAL}=e_i^T W_k e_j$$
$P(ijk)=\sigma(f)$

Factorization of the actual tensor: $Y_k \simeq E W_k E^T$
RESCAL is similar to recommendation systems and traditional tensor factorization
methods.

$W_k$: weight matrix for linear interactions.
This is a bilinear model.
"Good actors are likely to receive prestigious rewards."
Training jointly learns weight matrices and entity embeddings.

\textit{Relational learning via shared representations}:
entities have the same latent representation as subject or object, and for every
relation.
\textit{Semantic embeddings}: can be used to e.g. cluster entities.
There are smart ways to train RESCAL in \cite{review-of-relational-ml-for-kgs}.

RESCAL is state-of-the-art for some relational learning tasks.

Needs to learn $H_e^2$ parameters per relation.

\subsection{E-MLP (entity-mlp)}

Hidden layer with nonlinearity and fewer neurons.
Needs to learn $H_a + (H_a \times 2H_e)$ parameters per relation.
We learn the interactions of latent features via $A_k$, while the tensor product
in RESCAL considers all possible interactions.
($H_a$ is the size of the hidden layer.)

\subsection{ER-MLP (+relation embedding)}

Layer 1: embedding of subject, object, relation. Uses one global weight matrix
for all relations. This model was used in Knowledge Vault\ref{knowledge-vault}.
"Semantically close" relations (e.g., child, parent, spouse) are close in latent
space\cite{knowledge-vault}.

NTN\ref{neural-tensor-networks-for-kbc} is even bigger and tends to overfit.

\subsection{Latent distance models}

Also "latent space models" in social network analysis. Entities are likely to be
in a relationship if their latent representations are close in some distance
measure.

For uni-relational data, one proposed model: $f(e_i,e_j)=-d(e_i,e_j)$ where $d$
is arbitrary distance measure (e.g. Euclidean metric).

TODO: How would one train this?

\paragraph{Structured embeddings}
\label{structured-embeddings}

Article: \cite{structured-embeddings}

$f_{ijk}^{SE}=-\|A_k^s e_i - A_k^o e_j\|_1$ where $A_k=[A_k^s; -A_k^o]$.
Learned using "ranking loss": entities in existing relationships are closer than
in non-existing relationships.

Number of parameters: $2N_r H_e H_a + N_e H_e$

\paragraph{TransE}
\label{transe}

Article: \cite{transe}

Reduces number of parameters over SE.
Latent features are transformed by relation-specific offset:
$f_{ijk}^{TransE}=-d(e_i+r_k,e_j)$.
Inspired by \cite{efficient-word-representation-estimation} - "queen - woman
+ man = king" (TODO: read the paper)
(Can be nicely rewritten under some assumption --
\cite{review-of-relational-ml-for-kgs}.

\paragraph{Comparing the models}

In \cite{knowledge-vault}, ER-MLP outperformed NTN.
In \cite{embedding-entities-and-relations}, more extensive experiments showed
RESCAL to be best.
So, the best model is dataset-dependent.

TODO: cite

TODO

\subsection{Graph feature models}

\paragraph{Similarity models for uni-relational data}

Local similarity indices: \textit{Common Neighbors}, \textit{Academic-Adar
index}, \textit{Preferential Attachment}: similarity based on number of common
neighbors or absolute number of neighbors. Local computation is fast, depends
only on direct neighbors, but it's too localized to capture long-distance
relationships.

Global similarity indices: Katz index, Leicht-Holme-Newman indx: similarity from
ensemble of all paths between. Hitting Time, Commute Time, PageRank: from random
walks on graph. Often better predictions, but more expensive than local.

Quasi-local: Local Katz index, Local Random Walks (bounded-length): trade-off.

\paragraph{Rule mining, inductive logic programming}

TODO: section V-B of \cite{review-of-relational-ml-for-kgs}

See \ref{path-ranking-algorithm}.

\subsection{Combining the models}

Knowledge Vault shows that neither graph models neither latent feature models
are strictly superior -- rather, their strengths are complementary.

Knowledge Vault trains them separately and then fits binary classifier
(\textit{stacking} -- [121] in \cite{review-of-relational-ml-for-kgs}).
Problem: models don't specialize/cooperate.

\paragraph{Additive relational effects model}
From \cite{reducing-the-rank}:
$f_{ijk}^{RESCAL+PRA}=w_k^{(1)T}\phi_{ij}^{RESCAL} +
w_k^{(2)T}\phi_{ijk}^{PRA}$,
trained by alternatively optimizing RESCAL parameters with PRA parameters.
RESCAL only has to model "residual errors" left behind by PRA, which enables
lower latent dimensionality. The resulting model has better accuracy.

TODO: Get results.

TODO: Other combined models -- see VI-B in
\cite{review-of-relational-ml-for-kgs}.

\subsection{Training on KGs}

Maximum a posteriori (MAP) (== regularized maximum likelihood) on observed
triples

Issues with selecting negative samples. Solutions: pertubing (type-constrained),
LCWA.

\paragraph{Pairwise loss training}
We want the probability to be higher for positive samples than assumed-negative
samples:
$$\min_\Theta \sum_{x^+\in D^+}\sum_{x^-\in
D^-}L(f(x^+;\Theta),f(x^-;\Theta))+\lambda reg(\Theta)$$
$L(f,f')$ is margin-based ranking loss function such as: $\max(1+f'-f,0)$
Easy to optimize by stochastic gradient descent: pick one positive, one negative
sample. Can take long time to converge.

\paragraph{Model selection} Evaluation criteria: area under ROC (AUC-ROC -- true
positive rate / false positive rate), area under precision-recall curve (AUC-PR)

\subsection{Markov random fields}

We drop conditional independence, each $y_{ijk}$ can depend on any others.
To reduce number of potential dependencies, use template-based graphical models.

Each $y_{ijk}$ is a node in a \textit{dependency graph}.

Markov Random Fields (\textit{undirected} graphical models):
Markov networks are undirected and may be cyclic.
$P(Y|\Theta) \sim \prod_c \psi(y_c|\Theta)$, $\psi\geq 0$ potential
function on $c$-th clique in dependency graph. (Local potentials have no
probabilistic interpretation, unlike directed graphical models.)

Markov logic. Given set of formulae, create edge in dependency graph if
corresponding facts occur in at least one grounded formula.
Define $\psi(y_c|\Theta)$ such that $P(Y|\Theta)=\frac{1}{Z}\prod_c
exp(\Theta_c x_c)$, where $x_c$ is number of true groundings of $F_c$ in $Y$,
$\Theta_c$ is weight for formula $F_c$. $\Theta_c>0$: prefer $F_c$ to be true,
$<0$: prefer $F_c$ to be false.

Process of generating MRF graph: \textit{grounding} or \textit{instantiation}.

Inference problem: estimate most probable configuation $y^*$, or posterior
marginals $p(y_i|\Theta)$. Intractable, so we need to use heuristics.
Gibbs sampling, MC-SAT ([128]), MPLP (max-product linear programming) to get
MAP estimate.

Learning problm: specify the potential functions and values of $\Theta$.
Often: logical rules. (Structure learning is then equivalent to rule learning.)
Parameter estimation is expensive, since it needs to call inference as a
subroutine.

TODO

\section{Knowledge Vault}
\label{knowledge-vault}

\textit{Knowledge Vault}\cite{knowledge-vault} is Google's project of extending
its proprietary Knowledge Graph with relations extracted by reasoning over
the knowledge graph and from a corpus of plain text, HTML pages and HTML
tables. Because the sources are noisy, each relation has an associated
confidence score (an estimate of the probability that the relation actually
holds).

Knowledge Vault has 1.6B triples, of which 324M have a confidence of 0.7 or
higher, and 271M have a confidence of 0.9 or higher.
This is 38x more than previous comparable system - DeepDive.
About 1/3 of the 271M confident triples were not previously in Freebase, so
the noisy sources are actually adding new relations.

Knowledge Vault consists of several \textit{extractors}. Each extractor runs on a
document and yields triples and confidence scores, representing uncertainty
about the identity of the relation and its arguments. The confidence score is
the probability of the relation being true, conditional on features seen by
the extractor.
For each possible triple, Knowledge Vault also learns its \textit{prior
probability}, conditional only on information present in the source knowledge
graph.
The final confidence scores are calculated by \textit{knowledge fusion},
which combines extractor outputs with prior probabilities.
Confidence scores from extractors are calibrated to fit the actual probabilities
with Platt scaling\ref{platt-scaling}.
(An alternative would be using isotonic regression\ref{isotonic-regression}.)

Priors are probabilities of a relation being true, conditional
on the prior knowledge graph. They are useful for checking the consistency of
facts extracted from unreliable sources with known true facts in the knowledge
graph.

Evaluation protocol: 1) extract 1.6B candidates, 2) split them into test/train

TODO: Why are there fewer confident facts in Knowledge Vault than in Freebase?
Confident facts:

Freebase: 1500 entity types, 40M instances, 35000 relation types, 637M
facts.
Knowledge Graph: 1500 entity types, 570M instances, 35000 relation types, 18000M
facts.
Knowledge Vault: 1100 entity types, 45M instances, 4469 relation types, 271M
facts.

\paragraph{Local closed world assumption}
\label{lcwa}
To prepare training data including negative samples, Knowledge Vault uses the
\textit{local closed world assumption}: e.g., if the set of actors in a movie listed
in Knowledge Graph is non-empty, it is assumed to be complete and all other
actors are used as negative samples.
They tested the assumption and LCWA labels are not too far off.

\paragraph{Extractors}
The text extractor performs entity recognition, POS tagging, dependency parsing,
co-reference resolution (within one document) and entity linkage (from mentions
to KG entities).
Then, a binary classifier (using logistic regression) is trained for each of
4469 relation types using distant supervision\ref{distant-supervision}.
For the HTML extractor, similar features are used, augmented with DOM
information.
The remaining two extractors find relations in HTML tables and in webpages
with microformats or schema.org metadata.

For each relation, the output of all extractors is fused into one probability
conditional on the features used by all extractors. This is performed by
logistic regression, whose inputs are confidence scores of each extractor and
the number of sources used by each extractor, represented as its square root.
The classifier can learn the relative reliabilities of each extractor.
They tried logistic regression, and got a better result with boosted decision
stumps\ref{boosted-decision-stumps}.

TODO: RESULTS of Knowledge Vault

Two algorithms were used to predict prior probabilities.
One of them is PRA \ref{path-ranking-algorithm}.

\subsection{Neural network models}

\paragraph{Simple model}
Low-rank decomposition of 3d $E\times P\times E$ matrix. Learn $K$-dimensional
latent vectors, two for each entity ($u_s$ and $v_o$) and one for each relation
($w_p$). $K=60$. (Equivalent to PARAFAC method of tensor decomposition +
sigmoid.)
$$\Pr[G(s,p,o)=1] = \sigma(\sum\limits_{k=1}^K u_{sk} w_{pk} v_{ok})$$

\paragraph{Neural tensor networks\ref{neural-tensor-networks-for-kbc}}
Associate a different tensor with each relation.
$$\Pr[G(s,p,o)=1] = \sigma(\vec{\beta}_p f[\vec{u}_s^T \vec{W}_p^{1:M} \vec{v}_o])$$
$\vec{\beta}_p\in\R^K$, $\vec{W}_p^m\in\R^{K\times K}$.
Number of parameters: $\O(KE+K^2 MP)$ ($M$: number of "layers" in $W_p$ tensors)

\paragraph{The one they actually used}
One vector per predicate, then MLP to capture interaction terms.
$$\Pr[G(s,p,o)=1]=\sigma(\vec{\beta}^T f[\vec{A} [\vec{u}_s; \vec{w}_p, \vec{v}_o] ])$$
$\vec{A}\in\R^{L\times 3K}$.
$L$ second-layer weights in $\vec{\beta}$.
Parameters: $\O(L+LK+KE+KP)$, essentially same performance as [37].

Neural network models have essentially same AUC as PRA
(0.882 for NN, 0.884 for PRA).
The outputs of both systems for estimating the prior probability are then
fused. An extra feature is a 0/1 flag specifying whether the given prior system
could make a prediction.

Finally, the priors and the extracted confidence scores are again fused.

\paragraph{Proposed extensions}
Modeling mutual exclusion, modeling soft correlations
(e.g., most people have <=10 kids), multiple levels of abstraction (born in
Honolulu vs. born in Hawaii), discounting correlated sources, temporarily true
facts, adding new entities and relation types.

\section{Boosted decision stumps}
\label{boosted-decision-stumps}
A decision stump is a one-level decision tree, a type of weak learner
well suited for boosting (for instance, by AdaBoost).

\section{Platt scaling}
\label{platt-scaling}

Platt scaling\cite{platt-scaling} is a technique that scales the output $x$ of a binary classifier
to the probability the sample belongs to the positive class. It is used as a
calibration step for models that do not automatically learn to give good
probability estimates.

The probability estimated by Platt scaling that the sample $x$ belongs to the
positive class is:
$$\Pr[y=1|x] = \frac{1}{1+\exp(Ax+B)}$$
$A$ and $B$ are parameters trained on a held-out calibration set.
Platt suggested learning them using Levenberg-Marquardt.

\section{Isotonic regression}
\label{isotonic-regression}
TODO

https://en.wikipedia.org/wiki/Isotonic\_regression

\section{$F_1$ score}
\label{f-1}
TODO: Why is it useful as a measure?

$$F_1 = 2\cdot\frac{prec \cdot recall}{prec + recall}$$

\section{Knowledge graph refinement survey\cite{kg-refinement-survey}}

Most existing work for KG refinement runs on DBpedia, YAGO, or Freebase.

It's useful to study refinement isolated from construction -- better
understanding and separation of effects, generality over KGs.

The authors classify KG refinement methods as \textit{completion}, or
\textit{error correction}. Completion enhances completeness, correction
reduces error.
Some approaches are also \textit{targeted} to correct only some part of the
knowledge graph, e.g. one relation type (like entity types), knowledge graph
intelinks or literal values.
Some methods try to extends the knowledge graph's \textit{schema}.
\textit{Internal} methods only operate on the input knowledge graph, while
\textit{external} methods use additional data, like text corpora.

There are several approaches to evalute the methods.
\paragraph{KG as silver standard}
KG completion can use the KG itself as a \textit{silver standard}: we check how
well can we predict existing relations in the KG if we hide them from the
algorithm. Silver standard is not suitable to measure error detection, because
it assumes the KG to be correct.
A slight problem is that the entire KG is used as input to completion and then
also for evaluation, but it would be hard to split it into a "train" and "test"
set, so most work uses the entire KG anyway.

The measured metrics are usually recall, precision and F-measure\ref{f-1}.

Another problem is that most KGs are \textit{open world}, and if the method
correctly completes a new edge that's missing in the knowledge graph, it would
count as a negative sample and lower the precision. To obtain better negative
samples, we can use the \textit{local closed world assumption}\ref{lcwa}.

\paragraph{Partial gold standard}
We can also use a \textit{partial gold standard}: take a part of the KG and
manually fill in every relation that "should be there" (for completion), or
manually label a set of relations as correct or incorrect (for correction).
Completion is usually again compared by recall, precision and $F_1$.
Correction uses accuracy and/or area under ROC curve (AUC).
Gold standards can be collected from human experts, or we can use another
interlinked KG as a partial gold standard (through 20\% of interlinks between
DBpedia and Freebase are incorrect; ~50\% of owl:sameAs connect similar, but not
exactly same entities).

\paragraph{Ex post evaluations}
Completions or corrections are given to human raters to label as correct or
incorrect. Useful, for example, when partial gold standard from high-quality KG
is used to evaluate correction methods, the standard might not have
meaningfully many errors.

\subsection{KG completion methods}
Internal methods use only the KG itself as input.

\paragraph{Internal - Classification}
Supervised classification can be used to predict entity types (sometimes
\textit{multi-label} - e.g. Arnold Schwarzenegger is a Politician and Actor).
The features used for classification are usually relations connecting to other
entities. In hierarchical ontologies, type prediction is a \textit{hierarchical
classification} problem (through there have been no applications of it so far).
Classification can be used also to predict the existence of a relation between
a pair of entities.
See: \ref{neural-tensor-networks-for-kbc}

TODO: read this paragraph again. figure out what exactly's going on here.

\paragraph{Probabilistic and statistical methods}
SDType: predicts the type of an entity based on graph features (e.g., lots of
"cast" nodes mean high likelihood it's an Actor). Deployed on DBpedia, adds 3.4
million type statements.
ProSWIP: predicts entity types.

Statistical methods to enrich schema with additional domains/ranges of
relations, disjointness axioms, ...

Oren et al. [58]: predicts relation, but not which entity it connects to.

[60]: Predict missing types in DBpedia based on redundancies in type systems
(YAGO vs. DBpedia ontology -- based on infoboxes vs. categories).

Association rule mining to extend schemas (e.g., domain/range restrictions,
disjointness axioms, ...) [80, 20]

\paragraph{External methods}
External methods use data sources external to the KG (e.g., knowledge graph
interlinks, Wikipedia, text corpora, ...).

\paragraph{External - Classification}
[57]: predict entity types by k-NN on Wikipedia link graph.
There are typically more interlinks between Wikipedia pages than KG entities.
[3]: different-language DBpedias to predict missing types.

\paragraph{External - NLP-based methods}
Text-based methods for predicting types and relations.
Types: Hearst patterns[28] ("ISA") in abstracts ([23, 36]).
[38] learns patterns on Wiki abstract using conditional random fields [37].
On entire articles: [86]

Prediction of relation between two entities: usually by \textit{distant
supervision}\ref{distant-supervision}.

\paragraph{External - information extraction from semi-structured data}
For example, some methods leverage extra tags in Wikipedia.
This is usually combined with DBpedia.

[51] extract information from tables in Wikipedia.
For two entities co-occurring in a table, they likely share a relation.
They first collect all relations that could possibly hold between at least 1
pair of entities, then train a classifier to predict which relation actually
holds.

[65] proposes the use of List pages in Wikipedia to generate type and relation
assertions (e.g., "List of Jewish-American Writers").

\paragraph{Knowledge Graph Fusion based on interlinks}
E.g.: different DBpedia languages ([10]). [18]: probabilistic mapping between
KGs.

\subsection{Error detection methods}
No methods combine completion and correction. The only exception is SDType and
SDValidate (they can output both errors and completion axioms).

\paragraph{Internal - Classification}
Knowledge Vault uses binary classification to tell which relations should hold,
as a cleaning step after knowledge extraction.

\paragraph{Internal - Reasoning}
Reasoning is a semantic web technique that finds whether a set of assertions is
free of contradictions (based on a rich ontology).
NELL and PROSPERA use reasoning to determine whether a proposed new axiom is
plausible. Real KGs have to deal with errors and noise.

Many KGs - e.g., DBpedia - don't have enough richness for useful reasoning
(e.g., disjointness assertions). We need to enrich such ontologies to let
reasoning find errors -- e.g. [78] uses statistical methods; [39] association
rule mining; [46] inductive logic programming.

\paragraph{Internal - Outlier detection}
Numeric literals are a natural target. ([85]: applied to DBpedia; most found
outliers are actual errors; extra split -- e.g. population, for countries,
cities, towns in isolation)

Outlier detection can be used to find suspicious interlinks -- e.g., using
entity types in both KGs as features.
Techniques: Local Outlier Factor [9], cluster-based outlier detection [27].

[63]: characteristic distribution of subject/object types for every relation

Graph-based measures (degree, clustering coefficient, centrality, ...) to
identify wrong KG interlinks [25] -- compares expected to actual distribution
(e.g., power-law-like for degree of entities).

\paragraph{External}
External methods for error detection are few.
DeFacto[40] checks facts in DBpedia by lexicalizing them to natural language
sentences and searching the Web ofr them. Sentences with few hits are assigned
a low confidence score.

\subsection{Findings}

\paragraph{Methods}

\paragraph{No combining of error detection and completion}
Only SDType/SDValidate combines error detection and completion (the algorithms
are highly similar and output either completion axioms, or errors).
In principle, we could use many approaches to do both (e.g., find relations
which have no supporting statements in corpus), but they don't.
Boosting weights, random forest attribute weights, ... could be used to develop
a system for both.

Hardly any error-detecting mechanisms also say how to correct the error.

Most approaches also focus only on one target -- rarely on e.g. both type and
relation assertions.

\paragraph{Adding new entities}
No examined approaches try to \textit{add new} entities. Entity set expansion
methods have been deeply investigated in NLP. ([59, 72, 83])
This might be interesting to apply to long-tail entities.
Example: what if there's a missing page in Wikidata? Wikipedia has fairly high
standards for inclusion.

Many methods are not genuinely graph-based. For example, we could do graph-based
outlier detection or association rule mining ([2, 35]) or graph kernel functions
(e.g. for SVMs) for RDF graphs ([33, 44, 15]).

\paragraph{Evaluation methodologies}

Most papers use precision and recall (or, post-hoc, precision and number of
extracted statements). Others: ROC curves, accuracy, root mean squared error,
...
KG as silver standard, ex-post evaluations and partial gold standards appear
with equal frequency. Ex-post evaluations are mostly used for error detection.

Partial gold standards are usually publicly available, which allows for
replication and comparison.

The major KG used is DBpedia. One problem is that DBpedia is republished every
year.

Roughly 2/3 of approaches evaluated on DBpedia are evaluated \textit{only} on
DBpedia. Roughly 1/2 of approaches are evaluated on only one knowledge graph.
Some work only on DBpedia \textit{by design}.

The question of computational performance and scalability is often neglected.

It would be useful to have a common selection of benchmarks, as in, e.g.,
question answering.

There are few holistic solutions which try to improve the entire KG at once.

\section{Distant supervision}
\label{distant-supervision}
Distant supervision\cite{distant-supervision} predicts the existence of
relations based on positive and negative samples of relations and a large text
corpus. First, Named Entity Recognition is performed on the corpus to find KG
entities.
Then, Reference Resolution resolves references within each document.

Distant supervision is trained to predict the occurence of one relation,
e.g. IsCapitalOf.

If entities X and Y are in the IsCapitalOf, each sentence that contains both
entity X and entity Y is presumed to potentially contain information about
the IsCapitalOf relation existing between X and Y.
For all positive and negative samples, we find all sentences containing the two
entities, transform the sentences (and the entity mentions) into a feature
vector and train a binary classifier.
The binary classifier is then run on the entire corpus to predict unknown
relations.

The negative samples can be selected
by randomly sampling a pair of entities with the proper type that don't have the
IsCapitalOf relation. We assume that this is a true negative, not a missing
relation in the knowledge graph.

The main benefit of distant supervision is that it removes the need to manually
label sentences as samples of relations.

The features fed into the binary classifier (e.g., logistic regression)
include:
\begin{itemize}
\item The sequence of words between the two entities.
\item The POS tags of this sequence.
\item A flag indicating which entity came first.
\item A window of $k$ words to the left of entity 1 and right of entity 2.
\item The dependency path between the two entities in the parse.
\item For each entity, one "window" node not part of the dependency path.
	(A node connected to one of the two entities and not part of the path.)
		-- generate 1 conjunctive feature for each pair of left, right
		window nodes, and features which omit one or both of them
	\cite{distant-supervision}
\item Conjunction features.
\end{itemize}

If we find multiple sentences containing the pair of entities for which we want
to predict a relation, we can feed the union of all their features into
the classifier. This lets us combine weak evidence from different sentences.
("Spilberg's film X" can be "film director" or "film producer", "X, directed by
Spilberg" can be "organization director" or "film director").

in both cases: Wikipedia is the corpus

TODO: kb-completion-via-search-based-qa

TODO: distant-supervision-noise-reduction

\subsection{Applied to extend DBpedia coverage}
Article:
\cite{extending-dbpedia-coverage-using-distant-supervision-over-wikipedia}
Part of wider effort to extend DBpedia -- other articles add classes and
automatic mappings to other languages.

TODO: Modeling relations and their mentions without labeled text -- better
dealing with noise; can't be dowloaded.

DBpedia comes from infoboxes but more than 50\% of Wiki articles don't have one.

Assumption: information present in the infobox is also expressed somewhere in
the article text. The tool used is jSRE (http://htl.fbk.eu/en/technology/jSRE).
The results were tested on 7 DBpedia properties.
JWPL (https://code.google.com/p/jwpl/) was used to produce plain text from
article source code (by removing tables, images and wiki markup). Stanford
CoreNLP was used to perform tokenization, sentence splitting, POS tagging,
lemmatization and Named Entity Recognition. (CoreNLP's NER module annotates
persons, organizations, locations, numbers and dates. CoreNLP's MISC tag was
used for other DBpedia classes.)

Given a DBpedia relation (e.g. Barack Obama was born in Honolulu), they look for
sentences containing entities of the correct type for both domain and range
where the corresponding strings match. They use different matching strategies:
1) exact match, 2) deleting parts between brackets (e.g., "Carrie (novel)"), 3)
"John Fitzgerald Kennedy" => "John", "John Kennedy", ... -- any combination
of tokens in the right order. Numerical values are matched if their relative
difference is less than 5\%.

Negative sample sentences are collected by looking for sentences that contain
the domain entity and an entity with the same type as the range entity different
from the actual value. (e.g., Obama and Los Angeles would be negative for "was
born in") This works well where there is no ambiguity in the relation.

Extra strategies:

\paragraph{"Positives cannot be negatives"}: If you already used a sentence as a
positive sample, don't use it as a negative sample as well.

\paragraph{"Only one sentence per relation"}: Discard all pages where there is
more than one sentence that has more than 1 sentence with both the domain and
the range entity.

\paragraph{"Only one relation for each value"}: If there's more than 1 relation
sharing the same domain and range, remove all (e.g., "Mark Zuckenberg" worked
for / founded "Facebook") -- we cannot disambiguate one from the other

jSRE uses kernel methods to embed input data into a feature space. Uses SVMs
with "global context kernel" and "local context kernel". "Global context":
bag-of-words on before-between, between, between-after. Second: basic NLP
features -- lemma, POS, stem, capitalization, etc.
TODO: details?

TODO: I could do a bottleneck study on the ML pipeline. Would it be worth it to
improve the RE algorithm?

Without strategies:

(capital: precision 82\% recall 67\% -- also feasible on small sets)
birthDate: precision 92\% recall 100\%.
deathPlace: precision 51\% recall 61\%.
deathDate: precision 94\% recall 99\%.
headquarter: precision 71\% recall 84\%.

With strategies (increases $F_1$ score):

birthDate: precision 91\% recall 100\%.
deathPlace: precision 59\% recall 77\%.
deathDate: precision 93\% recall 100\%.
headquarter: precision 81\% recall 80\%.

TODO: [24] DBpedia + distant supervision for TAC-KBP slot-filling

Future work: add disambiguation tools, Wiki-specific features, coreference
resolution (e.g., "he"), multiple languages (distant supervision is
language-independent in principle).

\subsection{Applied to Freebase}

Article: \cite{distant-supervision}

10000 instances of 102 relations at precision 67.6\%. Tradeoff is uncertain.
Top 1000 instances: 91\% on place of birth, 63\% on nationality, 81\% on place
of death, 71\% on film-writer.

Lexical features, as in distant supervision paper. Window size: 0, 1, 2 --
conjunctive features. POS tagger: max-entropy trained on Penn Treebank,
simplified into 7 categories (noun, verb, adverb, adjective, number, foreign
word, anything else).

\section{Neural tensor networks for KBC}
\label{neural-tensor-networks-for-kbc}

http://www.socher.org/index.php/Main/ReasoningWithNeuralTensorNetworksForKnowledgeBaseCompletion

Reimplementation: https://github.com/dddoss/tensorflow-socher-ntn

Contributions:

\paragraph{Relations as NTNs}
Each entity is represented as a vector. Each relation is defined through the
parameters of a neural tensor network which can explicitly relate two entity
vectors.

\paragraph{Averaging word vectors}
The vector representing each entity is the average of its word vectors, which
allows weight sharing (e.g., Bank of China, China).
The word vectors are trained on large unlabeled text.

Outperforms [8,9,10]. [8,9] can also benefit from initializing with unsupervised
word vectors.

Each relation triple is described by a neural network. Pairs of database
entities are given as inputs.

$e_1, e_2\in\R^d$: vector representations of two entities.

Associate a different tensor with each relation.
$$g(R,\vec{e_1},\vec{e_2}) = \vec{\beta}_R f[\vec{e_1}^T \vec{W}_R^{1:M}
\vec{e_2} + V_R [\vec{e_1};\vec{e_2}]^T + b_R]$$
$\vec{\beta}_R\in\R^K$, $\vec{W}_R^m\in\R^{D\times D}, \vec{W}^{1:K}_R\in\R^{D\times
D\times K}$, $V_R\in\R^{K\times 2D}$, $b_R\in\R^K$.

The tensor multiplication adds multiplication terms.
The intuition is that each slide of the tensor is responsible for one type of
entity pair or instantiation of a relation. ("Mechanical entities - cars - have
parts."; "animals have parts")

\paragraph{Distance model}: $$g(R,\vec{e_1},\vec{e_2})=\|W_{R,1} \vec{e_1} -
W_{R,2} \vec{e_2}\|$$
Low distance here means "likely to have relation". One problem of this model
is that it allows no interaction between terms of the two vectors.

\paragraph{Single layer model} $$g(R,\vec{e_1},\vec{e_2})=u_R^T
f(W_{R,1}\vec{e_1} + W_{R,2}\vec{e_2})=u_R^T f([W_{R,1}
W_{R,2}] [\vec{e_1} \vec{e_2}]^T)$$
Non-linearity only provides a weak interaction.
(Special case of tensor network with tensor set to 0.)

\paragraph{Hadamard model}
Hadamard product: $(A\otimes B)_{ij}=A_{ij}B_{ij}$
$$g(R,\vec{e_1},\vec{e_2})=(W_1 \vec{e_1}\otimes W_{rel,1}\vec{e_R}+b_1)^T (W_2
\vec{e_2}\otimes W_{rel,2}\vec{e_R}+b_2)$$
The parameters $W_1,W_{rel,1},W_2,W_{rel,2}\in\R^{d\times d}$, $b_1,b_2\in\R^d$
are shared by all relations. It allows us to treat relations and entities in the
same way. Giving each relationship its own matrix operators gave increased
performance.

\paragraph{Bilinear model}
$$g(R,\vec{e_1},\vec{e_2})=\vec{e_1}^T W_R e_2$$
The model is restricted by dimensionality of embedding vectors. It can model
only linear interactions. It's a special case of NTNs with
$V_R=0,b_R=0,k=1,f=id$. For larger datasets, NTN has a larger expressive power
thanks to more layers.

Some exotic objective function.

All models work well with randomly initialized entity vectors.
A further improvement comes from representing entities by their word vectors
and pre-trained initialization.

$e_{homo sapiens} = \frac{1}{2}(e_{homo} + e_{sapiens})$, with backpropagation
to $e_{homo}$ representation.
In WordNet, 60\% entities only have a single word and $>90\%$ have $\leq 2$
words. RNNs did not give any distinct improvement over averaging.
Using $d=100$ vectors. Not dealing with homonyms.

Train/test split: removes "trivial" symmetries (if "A is similar to B" is in
train, then "B is similar to A" is not in test).

Neural tensor net gets 90\% accuracy on Freebase for predicting Person
attributes (e.g., gender, nationality). At least 2\% higher than bilinear model,
4\% higher than single layer model. (Institution, cause of death: hard, ~77\%.
Gender: ~97\%.)

\section{Plan}

\begin{itemize}
\item Replicate Knowledge Vault atop Wikidata
\item Distant supervision: try training a recursive neural network rather than
	logistic regression
\item Experiment with full materialization (e.g., born in Honolulu, Hawaii, USA)
\end{itemize}

TODO: TAC-KBP (Text Analysis Conference - Knowledge Base Population)
slot-filling competition

\section{Review of relational machine learning for KGs}
\cite{review-of-relational-ml-for-kgs}

Supervised relational learning (SRL): predicting missing edges, predicting
properties of nodes, clustering nodes based on connectivity patterns. Arises in
analyzing social netowrks and biological pathways. Can be applied to large-scale
KGs, hence focus on \textit{scalable} methods.

Statistical relational learning: third-order \textit{adjacency tensor},
$y_{ijk}=0$ or $1$ (interpretation of $0$ depends on open/closed/local-closed
world assumption), random variable. We estimate the joint distribution from a
subset of observed triples.

\paragraph{Statistical properties of KGs}: transitivity (born in Boston, Boston
is in USA -> born in USA), and also "softer" patterns. Example:
homophily: entities tend to be related to other entities with similar
characteristics. Blocks: "science fiction authors", "US movies", ...
Relational learning tries to exploit global and long-range dependencies:
citizenship of Leonard Nimoy depends on country of his birthplace.

Three ways to model correlations between $y_{ijk}$'s: 1) assume all are
conditionally independent given latent features associated with subject, object,
relation type and additional parameters (\textit{latent feature models}), 2)
assume all are conditionally independent given observed graph features and
additional parameters (\textit{graph feature models}), 3) assume local
interactions (\textit{Markov Random Fields})

Latent models are justified by features not present in observed data (e.g.,
"Alec Guiness got the Academy Award for being a good actor"). Latent features
are, however, typically hard to interpret.

\subsection{Extensions and future work}

\paragraph{Non-binary relations}

Unary relations: e.g. height of a person. Naturally represented by matrix.
[64]: joint tensor-matrix factorization approach -- learns from binary and unary
relations via shared latent representation. Need to modify likelihood function:
Bernoulli for binary, Gaussian for numeric, Poisson for count. [134]
(citations in \cite{review-of-relational-ml-for-kgs})

Higher-arity relations: typically represented by auxilliary node (CVT) saying
"Spock's acting in Star Trek".
YAGO2's extension of SPO triple format: (subject, predicate, object, time,
location) [27]. Some work on extracting higher-arity relations from natural
language [135].

Knowledge Vault's planned extension: temporarily true facts.

RESCAL can be easily generalized to higher arities. Same generalization possible
for neural networks.

\paragraph{Hard constraints}

Real-world data often violates hard constraints. Machine learning can be robust
in the face of contradictory evidence.
Deterministic dependencies: precompute all true triples that can be derived from
constraints and add them to KG before learning (e.g., Barack Obama was born in
Honolulu, Hawaii, USA, Earth -- \textit{materialization}).

RESCAL has been extended to respect type constraints, which reduces the needed
rank.
Possible to learn \textit{approximate}, observed type constraints.

Functional constraints, mutual exclusiveness.


